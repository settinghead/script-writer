#!/usr/bin/env python3
"""
Brainstorm optimization script using MIPROv2
Uses DSPy's most advanced optimizer for story idea generation quality improvement
Supports both flat (single-group) and grouped optimization approaches
"""

import mlflow
import sys
from copy import copy
from typing import List, Dict, Tuple
import dspy
from dspy.teleprompt import MIPROv2

from brainstorm_module import BrainstormModule, OptimizedBrainstormModule
from evaluators import StoryIdeaEvaluator, create_evaluation_metric, create_grouped_evaluation_metrics
from common import BrainstormRequest
from inspect_optimized_prompts import inspect_optimized_module, save_optimized_prompts

def create_training_examples() -> List[dspy.Example]:
    """Create diverse training examples for optimization using real genre system"""
    examples_data = [
        # å¥³é¢‘ - çˆ±æƒ…ç±»
        {"genre": "ç”œå® ", "platform": "æŠ–éŸ³", "requirements_section": "æµªæ¼«ç”œèœœçš„çˆ±æƒ…æ•…äº‹ï¼Œé€‚åˆå¹´è½»è§‚ä¼—"},
        {"genre": "è™æ‹", "platform": "å°çº¢ä¹¦", "requirements_section": "å……æ»¡æ³¢æŠ˜ã€ç—›è‹¦å’Œæƒ…æ„ŸæŒ£æ‰çš„çˆ±æƒ…æ•…äº‹"},
        {"genre": "éœ¸æ€»", "platform": "å¿«æ‰‹", "requirements_section": "é«˜å†·å‹éœ¸é“æ€»è£ï¼ŒèŠ‚å¥ç´§å‡‘"},
        
        # å¥³é¢‘ - è®¾å®šç±»
        {"genre": "ç©¿è¶Š", "platform": "æŠ–éŸ³", "requirements_section": "èº«ç©¿æˆ–é­‚ç©¿ï¼Œå¤ä»£ç°ä»£èƒŒæ™¯éƒ½å¯"},
        {"genre": "é‡ç”Ÿ", "platform": "å°çº¢ä¹¦", "requirements_section": "é‡ç”Ÿé¢˜æï¼Œå¤ä»‡æˆ–æ”¹å˜å‘½è¿"},
        {"genre": "é©¬ç”²", "platform": "å¿«æ‰‹", "requirements_section": "å¤šé‡èº«ä»½è®¾å®šï¼Œåè½¬æƒŠå–œ"},
        {"genre": "æ›¿èº«", "platform": "æŠ–éŸ³", "requirements_section": "çœŸå‡åƒé‡‘ï¼ŒåŒèƒèƒæ›¿æ¢"},
        
        # å¥³é¢‘ - å…¶ä»–ç±»å‹
        {"genre": "èŒå®", "platform": "å°çº¢ä¹¦", "requirements_section": "å¯çˆ±èŒå¨ƒï¼Œæ¸©é¦¨å®¶åº­"},
        {"genre": "å›¢å® ", "platform": "å¿«æ‰‹", "requirements_section": "è¢«å…¨å®¶å® çˆ±çš„è®¾å®š"},
        {"genre": "å¨±ä¹åœˆ", "platform": "æŠ–éŸ³", "requirements_section": "å¨±ä¹åœˆèƒŒæ™¯ï¼Œæ˜æ˜Ÿç”Ÿæ´»"},
        
        # ç”·é¢‘ - è®¾å®šç±»
        {"genre": "ç„å¹»", "platform": "å¿«æ‰‹", "requirements_section": "ä¿®ç‚¼æˆä»™ï¼Œå‡çº§æ‰“æ€ª"},
        {"genre": "æœ«ä¸–", "platform": "æŠ–éŸ³", "requirements_section": "æœ«æ—¥æ±‚ç”Ÿï¼Œä¸§å°¸é¢˜æï¼Œåˆ¶ä½œæˆæœ¬å¯æ§"},
        
        # ç”·é¢‘ - é€†è¢­ç±»
        {"genre": "æˆ˜ç¥", "platform": "å°çº¢ä¹¦", "requirements_section": "å¼ºè€…å½’æ¥ï¼Œå…µç‹é¢˜æ"},
        {"genre": "ç¥è±ª", "platform": "æŠ–éŸ³", "requirements_section": "ä¸€å¤œæš´å¯Œï¼Œç‚¹çŸ³æˆé‡‘"},
        {"genre": "èµ˜å©¿", "platform": "å¿«æ‰‹", "requirements_section": "èµ˜å©¿é€†è¢­ï¼Œæ‰®çŒªåƒè€è™"},
        {"genre": "é€†è¢­", "platform": "å°çº¢ä¹¦", "requirements_section": "å°äººç‰©æˆé•¿ï¼Œé©¬ç”²å¤§ä½¬"},
        {"genre": "é‡‘æ‰‹æŒ‡", "platform": "æŠ–éŸ³", "requirements_section": "è¶…èƒ½åŠ›ï¼Œç³»ç»Ÿé€‰ä¸­"},
        {"genre": "é«˜æ‰‹ä¸‹å±±", "platform": "å¿«æ‰‹", "requirements_section": "éšä¸–é«˜æ‰‹é‡å‡ºæ±Ÿæ¹–"},
        
        # ç”·é¢‘ - å…¶ä»–ç±»å‹
        {"genre": "ç¥åŒ»", "platform": "å°çº¢ä¹¦", "requirements_section": "åŒ»æœ¯é«˜è¶…ï¼Œæ‚¬å£¶æµä¸–"},
    ]
    
    # Create DSPy examples and configure with proper input fields
    configured_examples = []
    for data in examples_data:
        example = dspy.Example(**data)
        configured_example = example.with_inputs("genre", "platform", "requirements_section")
        configured_examples.append(configured_example)
    
    return configured_examples

def create_group_specific_training_examples(group_name: str) -> List[dspy.Example]:
    """Create training examples tailored for specific evaluation groups"""
    base_examples = create_training_examples()
    
    if group_name == "creativity":
        # Focus on genres that require high creativity and engagement
        creative_genres = ["ç©¿è¶Š", "é‡ç”Ÿ", "é©¬ç”²", "æ›¿èº«", "ç„å¹»", "æœ«ä¸–", "é‡‘æ‰‹æŒ‡"]
        return [ex for ex in base_examples if ex.genre in creative_genres]
    elif group_name == "feasibility":
        # Focus on practical, cost-effective genres
        practical_genres = ["ç”œå® ", "éœ¸æ€»", "èŒå®", "å›¢å® ", "å¨±ä¹åœˆ", "ç¥åŒ»"]
        return [ex for ex in base_examples if ex.genre in practical_genres]
    elif group_name == "content_quality":
        # Focus on genres requiring detailed storytelling and logical coherence
        quality_genres = ["è™æ‹", "ç©¿è¶Š", "é‡ç”Ÿ", "æˆ˜ç¥", "é€†è¢­", "é«˜æ‰‹ä¸‹å±±"]
        return [ex for ex in base_examples if ex.genre in quality_genres]
    else:
        # Return all examples for overall/flat optimization
        return base_examples

def generate_ideas_with_retry(module, request: BrainstormRequest, max_retries: int = 2):
    """Generate ideas with retry logic for JSON parsing failures"""
    for attempt in range(max_retries + 1):
        try:
            ideas = module(
                genre=request.genre,
                platform=request.platform,
                requirements_section=request.requirements_section
            )
            if len(ideas) == 0:
                if attempt < max_retries:
                    print(f"  JSONè§£æå¤±è´¥ï¼Œé‡è¯• {attempt + 1}/{max_retries}")
                    continue
                else:
                    print(f"  âŒ JSONè§£æå¤±è´¥ï¼Œè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåœæ­¢æ‰§è¡Œ")
                    sys.exit(1)
            return ideas
        except Exception as e:
            if attempt < max_retries:
                print(f"  ç”Ÿæˆå¤±è´¥ï¼Œé‡è¯• {attempt + 1}/{max_retries}: {e}")
                continue
            else:
                print(f"  âŒ ç”Ÿæˆå¤±è´¥ï¼Œè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                sys.exit(1)

def run_flat_optimization(auto_mode: str = "medium") -> Tuple[dspy.Module, List[dspy.Example]]:
    """Run flat (single-group) optimization - current approach"""
    print(f"ğŸš€ å¼€å§‹å¹³é¢ä¼˜åŒ– (æ¨¡å¼: {auto_mode}) - æ‰€æœ‰æŒ‡æ ‡ç»Ÿä¸€ä¼˜åŒ–")
    print("=" * 60)
    
    try:
        # Create training examples
        train_examples = create_training_examples()
        print(f"åˆ›å»ºäº† {len(train_examples)} ä¸ªè®­ç»ƒæ ·ä¾‹")
        
        # Create evaluator and metric (single overall metric)
        evaluator = StoryIdeaEvaluator()
        metric = create_evaluation_metric(evaluator)  # This uses the overall score
        
        # Configure MIPROv2 optimizer
        optimizer = MIPROv2(
            metric=metric,
            auto=auto_mode,
            max_bootstrapped_demos=4,
            num_threads=8,
            max_labeled_demos=4,
            verbose=True,
            track_stats=True,
            seed=42
        )
        
        print(f"é…ç½®å¹³é¢ä¼˜åŒ–å™¨å®Œæˆï¼Œå¼€å§‹è®­ç»ƒ...")
        
        # Compile the module
        base_module = BrainstormModule()
        compiled_module = optimizer.compile(
            base_module, 
            trainset=train_examples,
            requires_permission_to_run=False
        )
        
        print(f"âœ… å¹³é¢ä¼˜åŒ–å®Œæˆ!")
        return compiled_module, train_examples
        
    except Exception as e:
        print(f"âŒ å¹³é¢ä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("åœæ­¢æ‰§è¡Œ")
        sys.exit(1)

def run_grouped_optimization(auto_mode: str = "medium") -> Tuple[Dict[str, dspy.Module], List[dspy.Example]]:
    """Run grouped optimization - separate optimization for different evaluation aspects"""
    print(f"ğŸš€ å¼€å§‹åˆ†ç»„ä¼˜åŒ– (æ¨¡å¼: {auto_mode}) - åˆ†åˆ«ä¼˜åŒ–ä¸åŒè¯„ä¼°ç»´åº¦")
    print("=" * 60)
    
    try:
        # Create evaluator and grouped metrics
        evaluator = StoryIdeaEvaluator()
        grouped_metrics = create_grouped_evaluation_metrics(evaluator, use_single_group=False)
        
        optimized_modules = {}
        all_training_examples = []
        
        for group_name, metric in grouped_metrics.items():
            print(f"\nğŸ“Š ä¼˜åŒ–ç»„åˆ«: {group_name}")
            print("-" * 40)
            
            # Create group-specific or shared training examples
            if group_name in ["creativity", "feasibility", "content_quality"]:
                train_examples = create_group_specific_training_examples(group_name)
                print(f"  ä½¿ç”¨ {len(train_examples)} ä¸ªé’ˆå¯¹æ€§è®­ç»ƒæ ·ä¾‹")
            else:
                train_examples = create_training_examples()
                print(f"  ä½¿ç”¨ {len(train_examples)} ä¸ªé€šç”¨è®­ç»ƒæ ·ä¾‹")
            
            all_training_examples.extend(train_examples)
            
            # Configure optimizer for this group
            optimizer = MIPROv2(
                metric=metric,
                auto=auto_mode,
                max_bootstrapped_demos=3,  # Slightly fewer demos per group
                num_threads=6,
                max_labeled_demos=3,
                verbose=True,
                track_stats=True,
                seed=42 + hash(group_name) % 100  # Different seed per group
            )
            
            print(f"  å¼€å§‹ä¼˜åŒ– {group_name} ç»„...")
            
            # Compile module for this group
            base_module = BrainstormModule()
            compiled_module = optimizer.compile(
                base_module,
                trainset=train_examples,
                requires_permission_to_run=False
            )
            
            optimized_modules[group_name] = compiled_module
            print(f"  âœ… {group_name} ç»„ä¼˜åŒ–å®Œæˆ!")
        
        print(f"\nâœ… æ‰€æœ‰åˆ†ç»„ä¼˜åŒ–å®Œæˆ! å…±ä¼˜åŒ–äº† {len(optimized_modules)} ä¸ªç»„åˆ«")
        return optimized_modules, list(set(all_training_examples))  # Remove duplicates
        
    except Exception as e:
        print(f"âŒ åˆ†ç»„ä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("åœæ­¢æ‰§è¡Œ")
        sys.exit(1)

def evaluate_model_performance(module, test_examples: List[dspy.Example], name: str) -> Tuple[float, Dict[str, float]]:
    """Evaluate model performance on test examples, return overall score and detailed scores"""
    print(f"\nğŸ“Š è¯„ä¼° {name} æ¨¡å‹æ€§èƒ½")
    print("-" * 40)
    
    evaluator = StoryIdeaEvaluator()
    total_scores = []
    detailed_scores = {
        'novelty': [], 'feasibility': [], 'structure': [], 
        'detail': [], 'logical_coherence': [], 'genre': [], 'engagement': []
    }
    
    for i, example in enumerate(test_examples[:5]):
        try:
            request = BrainstormRequest(
                genre=example.genre,
                platform=example.platform,
                requirements_section=example.requirements_section
            )
            
            # Generate ideas with retry logic
            ideas = generate_ideas_with_retry(module, request)
            
            # Evaluate
            result = evaluator.evaluate(ideas, request)
            total_scores.append(result.overall_score)
            
            # Collect detailed scores
            detailed_scores['novelty'].append(result.novelty_score)
            detailed_scores['feasibility'].append(result.feasibility_score)
            detailed_scores['structure'].append(result.structure_score)
            detailed_scores['detail'].append(result.detail_score)
            detailed_scores['logical_coherence'].append(result.logical_coherence_score)
            detailed_scores['genre'].append(result.genre_score)
            detailed_scores['engagement'].append(result.engagement_score)
            
            print(f"  æ¡ˆä¾‹ {i+1} ({example.genre}): {result.overall_score:.1f}/10")
            
        except Exception as e:
            print(f"  âŒ æ¡ˆä¾‹ {i+1} è¯„ä¼°å¤±è´¥: {e}")
            print("åœæ­¢æ‰§è¡Œ")
            sys.exit(1)
    
    if total_scores:
        avg_score = sum(total_scores) / len(total_scores)
        
        # Calculate average detailed scores
        avg_detailed_scores = {}
        for metric_name, scores in detailed_scores.items():
            if scores:
                avg_detailed_scores[metric_name] = sum(scores) / len(scores)
            else:
                avg_detailed_scores[metric_name] = 0.0
        
        print(f"\n  å¹³å‡åˆ†æ•°: {avg_score:.1f}/10")
        print(f"  è¯¦ç»†åˆ†æ•°:")
        for metric_name, score in avg_detailed_scores.items():
            print(f"    {metric_name}: {score:.1f}/10")
        
        return avg_score, avg_detailed_scores
    else:
        print("  âŒ æ— æœ‰æ•ˆè¯„ä¼°ç»“æœ")
        sys.exit(1)

def evaluate_grouped_models(grouped_modules: Dict[str, dspy.Module], test_examples: List[dspy.Example]) -> Tuple[float, Dict[str, float]]:
    """Evaluate grouped models and average their scores"""
    print(f"\nğŸ“Š è¯„ä¼°åˆ†ç»„ä¼˜åŒ–æ¨¡å‹æ€§èƒ½")
    print("-" * 40)
    
    group_scores = {}
    group_detailed_scores = {}
    
    # Evaluate each group
    for group_name, module in grouped_modules.items():
        overall_score, detailed_scores = evaluate_model_performance(module, test_examples, f"åˆ†ç»„-{group_name}")
        group_scores[group_name] = overall_score
        group_detailed_scores[group_name] = detailed_scores
    
    # Calculate averaged final score
    if group_scores:
        final_avg_score = sum(group_scores.values()) / len(group_scores)
        
        # Average detailed scores across groups
        final_detailed_scores = {}
        metric_names = ['novelty', 'feasibility', 'structure', 'detail', 'logical_coherence', 'genre', 'engagement']
        
        for metric_name in metric_names:
            metric_scores = [scores.get(metric_name, 0) for scores in group_detailed_scores.values()]
            final_detailed_scores[metric_name] = sum(metric_scores) / len(metric_scores) if metric_scores else 0.0
        
        print(f"\nğŸ¯ åˆ†ç»„æ¨¡å‹æœ€ç»ˆå¹³å‡åˆ†æ•°: {final_avg_score:.1f}/10")
        print(f"  æœ€ç»ˆè¯¦ç»†åˆ†æ•°:")
        for metric_name, score in final_detailed_scores.items():
            print(f"    {metric_name}: {score:.1f}/10")
        
        return final_avg_score, final_detailed_scores
    else:
        print("  âŒ æ— æœ‰æ•ˆåˆ†ç»„è¯„ä¼°ç»“æœ")
        sys.exit(1)

def save_optimized_model(module, name: str, score: float, detailed_scores: Dict[str, float] = None):
    """Save optimized model with MLflow"""
    try:
        with mlflow.start_run(run_name=f"optimized_brainstorm_{name}"):
            # Log parameters
            mlflow.log_param("optimizer_type", name)
            mlflow.log_param("model_type", "BrainstormModule")
            
            # Log metrics
            mlflow.log_metric("average_score", score)
            
            # Log detailed scores if available
            if detailed_scores:
                for metric_name, metric_score in detailed_scores.items():
                    mlflow.log_metric(f"avg_{metric_name}_score", metric_score)
            
            # Log model with proper input example format
            input_example = {
                "genre": "éƒ½å¸‚çˆ±æƒ…",
                "platform": "æŠ–éŸ³",
                "requirements_section": "æµªæ¼«ç”œèœœçš„çˆ±æƒ…æ•…äº‹"
            }
            
            model_info = mlflow.dspy.log_model(
                module,
                artifact_path="model",
                input_example=input_example
            )
            
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_info.model_uri}")
            return model_info
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        print("åœæ­¢æ‰§è¡Œ")
        sys.exit(1)

def compare_optimization_approaches():
    """Compare flat vs grouped optimization approaches with baseline"""
    print("ğŸ†š ä¼˜åŒ–æ–¹æ³•å¯¹æ¯”æµ‹è¯•: åŸºçº¿ vs å¹³é¢ä¼˜åŒ– vs åˆ†ç»„ä¼˜åŒ–")
    print("=" * 70)
    
    # Create test examples (separate from training)
    test_examples = [
        dspy.Example(genre="å…ˆå©šåçˆ±", platform="æŠ–éŸ³", requirements_section="å¥‘çº¦å©šå§»ï¼Œæƒ…æ„ŸçœŸå®"),
        dspy.Example(genre="æ¶å¥³", platform="å°çº¢ä¹¦", requirements_section="æ¶æ¯’å¥³é…é€†è¢­ï¼ŒåŒé‡äººæ ¼"),
        dspy.Example(genre="æ®‹ç–¾å¤§ä½¬", platform="å¿«æ‰‹", requirements_section="æ®‹ç–¾å¤§ä½¬éšè—èº«ä»½"),
        dspy.Example(genre="åå®«", platform="æŠ–éŸ³", requirements_section="åå®«äº‰æ–—ï¼Œæƒè°‹è®¾è®¡"),
        dspy.Example(genre="å¤ä»‡", platform="å°çº¢ä¹¦", requirements_section="å¤ä»‡ä¸»é¢˜ï¼Œæƒ…èŠ‚ç´§å‡‘")
    ]
    
    results = {}
    detailed_results = {}
    
    # 1. Test baseline model
    print("\n1ï¸âƒ£ æµ‹è¯•åŸºçº¿æ¨¡å‹...")
    baseline_module = BrainstormModule()
    baseline_score, baseline_detailed = evaluate_model_performance(baseline_module, test_examples, "åŸºçº¿æ¨¡å‹")
    results["åŸºçº¿æ¨¡å‹"] = baseline_score
    detailed_results["åŸºçº¿æ¨¡å‹"] = baseline_detailed
    
    # 2. Test flat optimization
    print(f"\n2ï¸âƒ£ è¿è¡Œå¹³é¢ä¼˜åŒ–...")
    flat_module, _ = run_flat_optimization("medium")
    flat_score, flat_detailed = evaluate_model_performance(flat_module, test_examples, "å¹³é¢ä¼˜åŒ–")
    results["å¹³é¢ä¼˜åŒ–"] = flat_score
    detailed_results["å¹³é¢ä¼˜åŒ–"] = flat_detailed
    
    # Inspect and save flat model
    print(f"\nğŸ” æ£€æŸ¥å¹³é¢ä¼˜åŒ–ç»“æœ:")
    inspect_optimized_module(flat_module, "å¹³é¢ä¼˜åŒ–")
    save_optimized_prompts(flat_module, "flat_optimization")
    save_optimized_model(flat_module, "flat_optimization", flat_score, flat_detailed)
    
    # 3. Test grouped optimization
    print(f"\n3ï¸âƒ£ è¿è¡Œåˆ†ç»„ä¼˜åŒ–...")
    grouped_modules, _ = run_grouped_optimization("medium")
    grouped_score, grouped_detailed = evaluate_grouped_models(grouped_modules, test_examples)
    results["åˆ†ç»„ä¼˜åŒ–"] = grouped_score
    detailed_results["åˆ†ç»„ä¼˜åŒ–"] = grouped_detailed
    
    # Inspect and save grouped models
    print(f"\nğŸ” æ£€æŸ¥åˆ†ç»„ä¼˜åŒ–ç»“æœ:")
    for group_name, module in grouped_modules.items():
        inspect_optimized_module(module, f"åˆ†ç»„ä¼˜åŒ–-{group_name}")
        save_optimized_prompts(module, f"grouped_optimization_{group_name}")
        # Save individual group models
        group_score, group_detailed = evaluate_model_performance(module, test_examples[:2], f"åˆ†ç»„-{group_name}")  # Shorter eval for individual groups
        save_optimized_model(module, f"grouped_{group_name}", group_score, group_detailed)
    
    # Display final comparison results
    print("\nğŸ† æœ€ç»ˆå¯¹æ¯”ç»“æœ")
    print("=" * 70)
    print(f"{'æ–¹æ³•':<15} {'æ€»åˆ†':<10} {'æå‡':<10}")
    print("-" * 35)
    
    baseline_score_val = results["åŸºçº¿æ¨¡å‹"]
    for method_name, score in results.items():
        if method_name == "åŸºçº¿æ¨¡å‹":
            improvement = "åŸºå‡†"
        else:
            improvement = f"+{score - baseline_score_val:.1f}"
        print(f"{method_name:<15} {score:<10.1f} {improvement:<10}")
    
    # Display detailed comparison
    print(f"\nğŸ“Š è¯¦ç»†æŒ‡æ ‡å¯¹æ¯”")
    print("=" * 70)
    metric_names = ['novelty', 'feasibility', 'structure', 'detail', 'logical_coherence', 'genre', 'engagement']
    
    print(f"{'æŒ‡æ ‡':<15} {'åŸºçº¿':<8} {'å¹³é¢':<8} {'åˆ†ç»„':<8} {'æœ€ä½³':<8}")
    print("-" * 55)
    
    for metric_name in metric_names:
        baseline_val = detailed_results["åŸºçº¿æ¨¡å‹"].get(metric_name, 0)
        flat_val = detailed_results["å¹³é¢ä¼˜åŒ–"].get(metric_name, 0)
        grouped_val = detailed_results["åˆ†ç»„ä¼˜åŒ–"].get(metric_name, 0)
        
        best_val = max(baseline_val, flat_val, grouped_val)
        best_method = "åŸºçº¿" if best_val == baseline_val else ("å¹³é¢" if best_val == flat_val else "åˆ†ç»„")
        
        print(f"{metric_name:<15} {baseline_val:<8.1f} {flat_val:<8.1f} {grouped_val:<8.1f} {best_method:<8}")
    
    # Find overall best method
    best_method = max(results.items(), key=lambda x: x[1])
    print(f"\nğŸ¥‡ æœ€ä½³ä¼˜åŒ–æ–¹æ³•: {best_method[0]} (å¾—åˆ†: {best_method[1]:.1f})")
    
    return results, detailed_results

def main():
    """Main optimization workflow with approach comparison"""
    print("ğŸ§ª æ•…äº‹åˆ›æ„ç”Ÿæˆä¼˜åŒ–ç³»ç»Ÿ - å¹³é¢ vs åˆ†ç»„ä¼˜åŒ–å¯¹æ¯”")
    print("=" * 70)
    
    try:
        # Setup MLflow
        mlflow.set_experiment("Brainstorm_Optimization_Comparison")
        mlflow.dspy.autolog()
        
        # Run comprehensive comparison
        results, detailed_results = compare_optimization_approaches()
        
        print("\nâœ… ä¼˜åŒ–å¯¹æ¯”æµç¨‹å®Œæˆ!")
        print("\nğŸ“ ç»“æœæ€»ç»“:")
        print("1. åŸºçº¿æ¨¡å‹: æœªç»ä¼˜åŒ–çš„åŸå§‹æ¨¡å‹")
        print("2. å¹³é¢ä¼˜åŒ–: ä½¿ç”¨å•ä¸€ç»¼åˆæŒ‡æ ‡ä¼˜åŒ– (å½“å‰æ–¹æ³•)")
        print("3. åˆ†ç»„ä¼˜åŒ–: åˆ†åˆ«ä¼˜åŒ–åˆ›æ„æ€§ã€å¯è¡Œæ€§ã€å†…å®¹è´¨é‡ä¸‰ä¸ªç»„åˆ«")
        print("4. æŸ¥çœ‹ MLflow UI äº†è§£è¯¦ç»†è®­ç»ƒè¿‡ç¨‹å’Œæ¨¡å‹å¯¹æ¯”")
        print("5. ç¼“å­˜æœºåˆ¶é¿å…é‡å¤è¯„ä¼°ç›¸åŒå†…å®¹")
        
    except Exception as e:
        print(f"âŒ ä¸»æµç¨‹å‘ç”Ÿé”™è¯¯: {e}")
        print("ç¨‹åºç»ˆæ­¢")
        sys.exit(1)

if __name__ == "__main__":
    main()
