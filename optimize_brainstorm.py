#!/usr/bin/env python3
"""
Brainstorm optimization script using MIPROv2
Uses DSPy's most advanced optimizer for story idea generation quality improvement
Supports both flat (single-group) and grouped optimization approaches
Uses golden examples from /examples directory for high-quality training data
"""

import mlflow
import sys
import json
import os
from copy import copy
from typing import List, Dict, Tuple
from datetime import datetime
import dspy
from dspy.teleprompt import MIPROv2

from brainstorm_module import BrainstormModule
from evaluators import StoryIdeaEvaluator, create_evaluation_metric, create_grouped_evaluation_metrics
from common import BrainstormRequest, StoryIdea
from inspect_optimized_prompts import inspect_optimized_module, save_optimized_prompts

# CONFIGURATION: Set optimization mode
# Options: "flat" (current approach - single overall metric) or "grouped" (separate group optimization)
OPTIMIZATION_MODE = "flat"  # Change this to "grouped" to use grouped optimization

# CONFIGURATION: Number of test examples to evaluate (reduce for faster optimization)
MAX_TEST_EXAMPLES = 2  # Reduced from 5 to speed up evaluation



# Global variables for logging
LOG_DIR = None
CURRENT_STEP = 0

class ComprehensiveLogger:
    """Comprehensive file-based logger for optimization process"""
    
    def __init__(self, base_dir: str = "optimization_logs"):
        self.base_dir = base_dir
        self.run_dir = None
        self.step_counter = 0
        self.setup_logging_directories()
    
    def setup_logging_directories(self):
        """Setup logging directory structure"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        mode = OPTIMIZATION_MODE
        self.run_dir = os.path.join(self.base_dir, f"{mode}_optimization_{timestamp}")
        
        # Create directory structure
        directories = [
            "00_configuration",
            "01_training_data", 
            "02_golden_examples",
            "03_optimization_process",
            "04_evaluation_results",
            "05_final_models",
            "06_prompts_comparison",
            "07_error_logs",
            "08_performance_metrics"
        ]
        
        for dir_name in directories:
            os.makedirs(os.path.join(self.run_dir, dir_name), exist_ok=True)
        
        print(f"ğŸ“ æ—¥å¿—ç›®å½•åˆ›å»º: {self.run_dir}")
    
    def log_configuration(self, config: Dict):
        """Log optimization configuration"""
        config_file = os.path.join(self.run_dir, "00_configuration", "optimization_config.json")
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        print(f"âœ… é…ç½®å·²ä¿å­˜: {config_file}")
    
    def log_training_data(self, examples: List[dspy.Example], data_type: str):
        """Log training examples"""
        data_dir = os.path.join(self.run_dir, "01_training_data")
        
        # Save examples as JSON
        examples_data = []
        for i, example in enumerate(examples):
            example_dict = {
                "index": i,
                "genre": getattr(example, 'genre', 'N/A'),
                "platform": getattr(example, 'platform', 'N/A'),
                "requirements_section": getattr(example, 'requirements_section', 'N/A'),
                "has_expected_output": hasattr(example, 'ideas'),
            }
            if hasattr(example, 'ideas'):
                example_dict["expected_ideas"] = example.ideas
            examples_data.append(example_dict)
        
        json_file = os.path.join(data_dir, f"{data_type}_examples.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(examples_data, f, ensure_ascii=False, indent=2)
        
        # Save summary
        summary_file = os.path.join(data_dir, f"{data_type}_summary.txt")
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"Training Data Summary - {data_type}\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Total examples: {len(examples)}\n")
            f.write(f"Data type: {data_type}\n")
            f.write(f"Timestamp: {datetime.now()}\n\n")
            
            # Genre distribution
            genres = {}
            platforms = {}
            for example in examples:
                genre = getattr(example, 'genre', 'Unknown')
                platform = getattr(example, 'platform', 'Unknown')
                genres[genre] = genres.get(genre, 0) + 1
                platforms[platform] = platforms.get(platform, 0) + 1
            
            f.write("Genre Distribution:\n")
            for genre, count in sorted(genres.items()):
                f.write(f"  {genre}: {count}\n")
            
            f.write("\nPlatform Distribution:\n")
            for platform, count in sorted(platforms.items()):
                f.write(f"  {platform}: {count}\n")
        
        print(f"âœ… {data_type} è®­ç»ƒæ•°æ®å·²ä¿å­˜: {len(examples)} ä¸ªæ ·ä¾‹")
    
    def log_golden_examples(self, golden_examples: List[dspy.Example]):
        """Log golden examples with detailed analysis"""
        golden_dir = os.path.join(self.run_dir, "02_golden_examples")
        
        # Save detailed golden examples
        golden_data = []
        for i, example in enumerate(golden_examples):
            example_dict = {
                "index": i,
                "genre": getattr(example, 'genre', 'N/A'),
                "platform": getattr(example, 'platform', 'N/A'),
                "requirements": getattr(example, 'requirements_section', 'N/A'),
                "expected_content": getattr(example, 'ideas', [])
            }
            golden_data.append(example_dict)
        
        golden_file = os.path.join(golden_dir, "golden_examples_detailed.json")
        with open(golden_file, 'w', encoding='utf-8') as f:
            json.dump(golden_data, f, ensure_ascii=False, indent=2)
        
        # Create readable analysis
        analysis_file = os.path.join(golden_dir, "golden_examples_analysis.txt")
        with open(analysis_file, 'w', encoding='utf-8') as f:
            f.write("Golden Examples Analysis\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Total golden examples: {len(golden_examples)}\n")
            f.write(f"Analysis timestamp: {datetime.now()}\n\n")
            
            for i, example in enumerate(golden_examples):
                f.write(f"Golden Example {i+1}:\n")
                f.write(f"  Genre: {getattr(example, 'genre', 'N/A')}\n")
                f.write(f"  Platform: {getattr(example, 'platform', 'N/A')}\n")
                f.write(f"  Requirements: {getattr(example, 'requirements_section', 'N/A')}\n")
                if hasattr(example, 'ideas') and example.ideas:
                    content = example.ideas[0]
                    f.write(f"  Content Length: {len(content)} characters\n")
                    f.write(f"  Content Preview: {content[:100]}...\n")
                f.write("\n")
        
        print(f"âœ… é»„é‡‘æ ·ä¾‹åˆ†æå·²ä¿å­˜: {len(golden_examples)} ä¸ªæ ·ä¾‹")
    
    def log_optimization_step(self, step_name: str, data: Dict, step_type: str = "general"):
        """Log optimization step with details"""
        self.step_counter += 1
        step_dir = os.path.join(self.run_dir, "03_optimization_process", f"step_{self.step_counter:02d}_{step_name}")
        os.makedirs(step_dir, exist_ok=True)
        
        # Save step data
        step_file = os.path.join(step_dir, "step_data.json")
        step_data = {
            "step_number": self.step_counter,
            "step_name": step_name,
            "step_type": step_type,
            "timestamp": datetime.now().isoformat(),
            "data": data
        }
        
        with open(step_file, 'w', encoding='utf-8') as f:
            json.dump(step_data, f, ensure_ascii=False, indent=2)
        
        # Save step summary
        summary_file = os.path.join(step_dir, "step_summary.txt")
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"Optimization Step {self.step_counter}: {step_name}\n")
            f.write("=" * 50 + "\n")
            f.write(f"Step type: {step_type}\n")
            f.write(f"Timestamp: {datetime.now()}\n\n")
            f.write("Step Data Summary:\n")
            for key, value in data.items():
                if isinstance(value, (str, int, float, bool)):
                    f.write(f"  {key}: {value}\n")
                elif isinstance(value, (list, dict)):
                    f.write(f"  {key}: {type(value).__name__} with {len(value) if hasattr(value, '__len__') else 'N/A'} items\n")
                else:
                    f.write(f"  {key}: {type(value).__name__}\n")
        
        print(f"ğŸ“ ä¼˜åŒ–æ­¥éª¤ {self.step_counter} å·²è®°å½•: {step_name}")
        return step_dir
    
    def log_evaluation_results(self, results: Dict, test_name: str):
        """Log evaluation results"""
        eval_dir = os.path.join(self.run_dir, "04_evaluation_results")
        
        # Save results as JSON
        results_file = os.path.join(eval_dir, f"{test_name}_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # Save readable report
        report_file = os.path.join(eval_dir, f"{test_name}_report.txt")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"Evaluation Report: {test_name}\n")
            f.write("=" * 50 + "\n")
            f.write(f"Timestamp: {datetime.now()}\n\n")
            
            if "overall_score" in results:
                f.write(f"Overall Score: {results['overall_score']:.2f}/10\n\n")
            
            if "detailed_scores" in results:
                f.write("Detailed Scores:\n")
                for metric, score in results["detailed_scores"].items():
                    f.write(f"  {metric}: {score:.2f}/10\n")
                f.write("\n")
            
            # Log test cases if available
            if "test_cases" in results:
                f.write("Test Cases:\n")
                for i, case in enumerate(results["test_cases"], 1):
                    f.write(f"  Case {i}: {case.get('genre', 'N/A')} - Score: {case.get('score', 'N/A')}\n")
        
        print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜: {test_name}")
    
    def log_model_prompts(self, module, model_name: str):
        """Log model prompts and configurations"""
        prompts_dir = os.path.join(self.run_dir, "06_prompts_comparison")
        
        # Save prompts
        prompt_file = os.path.join(prompts_dir, f"{model_name}_prompts.txt")
        with open(prompt_file, 'w', encoding='utf-8') as f:
            f.write(f"Model Prompts: {model_name}\n")
            f.write("=" * 50 + "\n")
            f.write(f"Timestamp: {datetime.now()}\n\n")
            
            # Try to extract prompts from the module
            try:
                if hasattr(module, 'predictor'):
                    predictor = module.predictor
                    if hasattr(predictor, 'signature'):
                        f.write("Signature:\n")
                        f.write(str(predictor.signature) + "\n\n")
                    
                    if hasattr(predictor, 'lm') and hasattr(predictor.lm, 'history'):
                        f.write("Recent LM History:\n")
                        for i, entry in enumerate(predictor.lm.history[-3:]):  # Last 3 entries
                            f.write(f"Entry {i+1}:\n")
                            f.write(str(entry) + "\n\n")
                
                # Try to get the actual prompt template
                f.write("Module Structure:\n")
                f.write(str(type(module)) + "\n\n")
                
            except Exception as e:
                f.write(f"Error extracting prompts: {e}\n")
        
        print(f"ğŸ“ æ¨¡å‹æç¤ºè¯å·²ä¿å­˜: {model_name}")
    
    def log_error(self, error: Exception, context: str):
        """Log errors with context"""
        error_dir = os.path.join(self.run_dir, "07_error_logs")
        
        error_file = os.path.join(error_dir, f"error_{datetime.now().strftime('%H%M%S')}.txt")
        with open(error_file, 'w', encoding='utf-8') as f:
            f.write(f"Error Log\n")
            f.write("=" * 30 + "\n")
            f.write(f"Timestamp: {datetime.now()}\n")
            f.write(f"Context: {context}\n")
            f.write(f"Error Type: {type(error).__name__}\n")
            f.write(f"Error Message: {str(error)}\n\n")
            
            # Try to get traceback
            import traceback
            f.write("Traceback:\n")
            f.write(traceback.format_exc())
        
        print(f"âŒ é”™è¯¯å·²è®°å½•: {context}")
    
    def log_generated_examples(self, examples: List[str], context: str):
        """Log generated examples in a readable format"""
        examples_dir = os.path.join(self.run_dir, "08_performance_metrics")
        
        examples_file = os.path.join(examples_dir, f"generated_examples_{context}.txt")
        with open(examples_file, 'w', encoding='utf-8') as f:
            f.write(f"Generated Examples - {context}\n")
            f.write("=" * 50 + "\n")
            f.write(f"Timestamp: {datetime.now()}\n")
            f.write(f"Total examples: {len(examples)}\n\n")
            
            for i, example in enumerate(examples, 1):
                f.write(f"Example {i}:\n")
                f.write(f"Length: {len(example)} characters\n")
                f.write(f"Content: {example}\n")
                f.write("-" * 30 + "\n")
        
        print(f"ğŸ“ ç”Ÿæˆæ ·ä¾‹å·²ä¿å­˜: {context}")
    
    def create_final_summary(self):
        """Create final optimization summary"""
        summary_file = os.path.join(self.run_dir, "OPTIMIZATION_SUMMARY.txt")
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("Optimization Run Summary\n")
            f.write("=" * 50 + "\n")
            f.write(f"Mode: {OPTIMIZATION_MODE}\n")
            f.write(f"Timestamp: {datetime.now()}\n")
            f.write(f"Total steps: {self.step_counter}\n\n")
            
            f.write("Directory Structure:\n")
            f.write("â”œâ”€â”€ 00_configuration/     - Optimization settings\n")
            f.write("â”œâ”€â”€ 01_training_data/     - Training examples used\n") 
            f.write("â”œâ”€â”€ 02_golden_examples/   - Golden examples analysis\n")
            f.write("â”œâ”€â”€ 03_optimization_process/ - Step-by-step process\n")
            f.write("â”œâ”€â”€ 04_evaluation_results/   - Performance evaluations\n")
            f.write("â”œâ”€â”€ 05_final_models/         - Saved model artifacts\n")
            f.write("â”œâ”€â”€ 06_prompts_comparison/   - Model prompts\n")
            f.write("â”œâ”€â”€ 07_error_logs/           - Error logs\n")
            f.write("â””â”€â”€ 08_performance_metrics/  - Performance data\n\n")
            
            f.write("To analyze results:\n")
            f.write("1. Check 04_evaluation_results for performance metrics\n")
            f.write("2. Review 03_optimization_process for step details\n")
            f.write("3. Compare 06_prompts_comparison to see prompt evolution\n")
            f.write("4. Check 07_error_logs if any issues occurred\n")
            f.write("5. Look at 08_performance_metrics for generated examples\n")
        
        print(f"ğŸ“‹ æœ€ç»ˆæ€»ç»“å·²ä¿å­˜: {summary_file}")
        return summary_file

# Initialize global logger
logger = ComprehensiveLogger()

def load_golden_examples() -> List[dspy.Example]:
    """Load golden examples from /examples directory"""
    examples_dir = "examples"
    golden_examples = []
    
    if not os.path.exists(examples_dir):
        print(f"âŒ é»„é‡‘æ ·ä¾‹ç›®å½•ä¸å­˜åœ¨: {examples_dir}")
        return []
    
    # Platform mapping for different genres
    platform_mapping = {
        "ç”œå® ": "æŠ–éŸ³",
        "è™æ‹": "å°çº¢ä¹¦", 
        "å¤ä»‡": "å¿«æ‰‹",
        "ç©¿è¶Š": "æŠ–éŸ³",
        "é‡ç”Ÿ": "å°çº¢ä¹¦",
        "é©¬ç”²": "å¿«æ‰‹",
        "éœ¸æ€»": "æŠ–éŸ³",
        "æˆ˜ç¥": "å¿«æ‰‹",
        "ç¥è±ª": "æŠ–éŸ³",
        "èµ˜å©¿": "å°çº¢ä¹¦",
        "ç„å¹»": "å¿«æ‰‹",
        "æœ«ä¸–": "æŠ–éŸ³",
        "å¨±ä¹åœˆ": "å°çº¢ä¹¦",
        "èŒå®": "æŠ–éŸ³",
        "å›¢å® ": "å¿«æ‰‹"
    }
    
    # Load all JSON files from examples directory
    for filename in os.listdir(examples_dir):
        if filename.endswith('.json'):
            filepath = os.path.join(examples_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Extract genre info
                genre_path = data.get('genre_path', [])
                if len(genre_path) >= 3:
                    genre = genre_path[2]  # The specific genre type
                elif len(genre_path) >= 2:
                    genre = genre_path[1]  # Subcategory
                else:
                    genre = "å…¶ä»–"
                
                # Map to platform
                platform = platform_mapping.get(genre, "æŠ–éŸ³")
                
                # Create requirements from tags
                tags = data.get('tags', [])
                requirements = f"è¦æ±‚: {', '.join(tags[:5])}"  # Use first 5 tags
                
                # Create expected output - the golden example should generate similar content
                expected_ideas = [data['content']]
                
                # Create DSPy example with inputs and expected output
                example_data = {
                    "genre": genre,
                    "platform": platform, 
                    "requirements_section": requirements,
                    "ideas": expected_ideas  # This is the expected output
                }
                
                example = dspy.Example(**example_data)
                configured_example = example.with_inputs("genre", "platform", "requirements_section")
                golden_examples.append(configured_example)
                
                print(f"  åŠ è½½é»„é‡‘æ ·ä¾‹: {filename} -> {genre} ({platform})")
                
            except Exception as e:
                print(f"  âŒ åŠ è½½ {filename} å¤±è´¥: {e}")
                continue
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(golden_examples)} ä¸ªé»„é‡‘æ ·ä¾‹")
    return golden_examples

def create_synthetic_training_examples() -> List[dspy.Example]:
    """Create diverse synthetic training examples for optimization using real genre system"""
    examples_data = [
        # å¥³é¢‘ - çˆ±æƒ…ç±»
        {"genre": "ç”œå® ", "platform": "æŠ–éŸ³", "requirements_section": "æµªæ¼«ç”œèœœçš„çˆ±æƒ…æ•…äº‹ï¼Œé€‚åˆå¹´è½»è§‚ä¼—"},
        {"genre": "è™æ‹", "platform": "å°çº¢ä¹¦", "requirements_section": "å……æ»¡æ³¢æŠ˜ã€ç—›è‹¦å’Œæƒ…æ„ŸæŒ£æ‰çš„çˆ±æƒ…æ•…äº‹"},
        {"genre": "éœ¸æ€»", "platform": "å¿«æ‰‹", "requirements_section": "é«˜å†·å‹éœ¸é“æ€»è£ï¼ŒèŠ‚å¥ç´§å‡‘"},
        
        # å¥³é¢‘ - è®¾å®šç±»
        {"genre": "ç©¿è¶Š", "platform": "æŠ–éŸ³", "requirements_section": "èº«ç©¿æˆ–é­‚ç©¿ï¼Œå¤ä»£ç°ä»£èƒŒæ™¯éƒ½å¯"},
        {"genre": "é‡ç”Ÿ", "platform": "å°çº¢ä¹¦", "requirements_section": "é‡ç”Ÿé¢˜æï¼Œå¤ä»‡æˆ–æ”¹å˜å‘½è¿"},
        {"genre": "é©¬ç”²", "platform": "å¿«æ‰‹", "requirements_section": "å¤šé‡èº«ä»½è®¾å®šï¼Œåè½¬æƒŠå–œ"},
        {"genre": "æ›¿èº«", "platform": "æŠ–éŸ³", "requirements_section": "çœŸå‡åƒé‡‘ï¼ŒåŒèƒèƒæ›¿æ¢"},
        
        # å¥³é¢‘ - å…¶ä»–ç±»å‹
        {"genre": "èŒå®", "platform": "å°çº¢ä¹¦", "requirements_section": "å¯çˆ±èŒå¨ƒï¼Œæ¸©é¦¨å®¶åº­"},
        {"genre": "å›¢å® ", "platform": "å¿«æ‰‹", "requirements_section": "è¢«å…¨å®¶å® çˆ±çš„è®¾å®š"},
        {"genre": "å¨±ä¹åœˆ", "platform": "æŠ–éŸ³", "requirements_section": "å¨±ä¹åœˆèƒŒæ™¯ï¼Œæ˜æ˜Ÿç”Ÿæ´»"},
        
        # ç”·é¢‘ - è®¾å®šç±»
        {"genre": "ç„å¹»", "platform": "å¿«æ‰‹", "requirements_section": "ä¿®ç‚¼æˆä»™ï¼Œå‡çº§æ‰“æ€ª"},
        {"genre": "æœ«ä¸–", "platform": "æŠ–éŸ³", "requirements_section": "æœ«æ—¥æ±‚ç”Ÿï¼Œä¸§å°¸é¢˜æï¼Œåˆ¶ä½œæˆæœ¬å¯æ§"},
        
        # ç”·é¢‘ - é€†è¢­ç±»
        {"genre": "æˆ˜ç¥", "platform": "å°çº¢ä¹¦", "requirements_section": "å¼ºè€…å½’æ¥ï¼Œå…µç‹é¢˜æ"},
        {"genre": "ç¥è±ª", "platform": "æŠ–éŸ³", "requirements_section": "ä¸€å¤œæš´å¯Œï¼Œç‚¹çŸ³æˆé‡‘"},
        {"genre": "èµ˜å©¿", "platform": "å¿«æ‰‹", "requirements_section": "èµ˜å©¿é€†è¢­ï¼Œæ‰®çŒªåƒè€è™"},
        {"genre": "é€†è¢­", "platform": "å°çº¢ä¹¦", "requirements_section": "å°äººç‰©æˆé•¿ï¼Œé©¬ç”²å¤§ä½¬"},
        {"genre": "é‡‘æ‰‹æŒ‡", "platform": "æŠ–éŸ³", "requirements_section": "è¶…èƒ½åŠ›ï¼Œç³»ç»Ÿé€‰ä¸­"},
        {"genre": "é«˜æ‰‹ä¸‹å±±", "platform": "å¿«æ‰‹", "requirements_section": "éšä¸–é«˜æ‰‹é‡å‡ºæ±Ÿæ¹–"},
        
        # ç”·é¢‘ - å…¶ä»–ç±»å‹
        {"genre": "ç¥åŒ»", "platform": "å°çº¢ä¹¦", "requirements_section": "åŒ»æœ¯é«˜è¶…ï¼Œæ‚¬å£¶æµä¸–"},
    ]
    
    # Create DSPy examples and configure with proper input fields
    configured_examples = []
    for data in examples_data:
        example = dspy.Example(**data)
        configured_example = example.with_inputs("genre", "platform", "requirements_section")
        configured_examples.append(configured_example)
    
    return configured_examples

def create_training_examples() -> List[dspy.Example]:
    """Create combined training examples using both golden examples and synthetic examples"""
    print("ğŸ“š åŠ è½½è®­ç»ƒæ ·ä¾‹...")
    
    # Load golden examples first
    golden_examples = load_golden_examples()
    
    # Load synthetic examples
    synthetic_examples = create_synthetic_training_examples()
    
    # Combine them, prioritizing golden examples
    all_examples = golden_examples + synthetic_examples
    
    print(f"ğŸ“Š è®­ç»ƒæ ·ä¾‹ç»Ÿè®¡:")
    print(f"  - é»„é‡‘æ ·ä¾‹: {len(golden_examples)} ä¸ª")
    print(f"  - åˆæˆæ ·ä¾‹: {len(synthetic_examples)} ä¸ª") 
    print(f"  - æ€»è®¡: {len(all_examples)} ä¸ª")
    
    return all_examples

def create_group_specific_training_examples(group_name: str) -> List[dspy.Example]:
    """Create training examples tailored for specific evaluation groups"""
    # First get all examples (golden + synthetic)
    base_examples = create_training_examples()
    
    # Always include golden examples as they are high-quality
    golden_examples = load_golden_examples()
    
    if group_name == "creativity":
        # Focus on genres that require high creativity and engagement
        creative_genres = ["ç©¿è¶Š", "é‡ç”Ÿ", "é©¬ç”²", "æ›¿èº«", "ç„å¹»", "æœ«ä¸–", "é‡‘æ‰‹æŒ‡", "å¤ä»‡"]
        filtered_examples = [ex for ex in base_examples if ex.genre in creative_genres]
        # Always include golden examples for creativity training
        return golden_examples + filtered_examples
    elif group_name == "feasibility":
        # Focus on practical, cost-effective genres
        practical_genres = ["ç”œå® ", "éœ¸æ€»", "èŒå®", "å›¢å® ", "å¨±ä¹åœˆ", "ç¥åŒ»"]
        filtered_examples = [ex for ex in base_examples if ex.genre in practical_genres]
        return golden_examples + filtered_examples
    elif group_name == "content_quality":
        # Focus on genres requiring detailed storytelling and logical coherence
        quality_genres = ["è™æ‹", "ç©¿è¶Š", "é‡ç”Ÿ", "æˆ˜ç¥", "é€†è¢­", "é«˜æ‰‹ä¸‹å±±", "å¤ä»‡"]
        filtered_examples = [ex for ex in base_examples if ex.genre in quality_genres]
        # Golden examples are especially important for content quality
        return golden_examples + filtered_examples
    else:
        # Return all examples for overall/flat optimization
        return base_examples

def generate_ideas(module, request: BrainstormRequest, num_ideas: int = 2):
    """Generate ideas using DSPy module - pure DSPy approach"""
    ideas = []
    for i in range(num_ideas):
        prediction = module(
                genre=request.genre,
                platform=request.platform,
                requirements_section=request.requirements_section
            )
        # Extract StoryIdea from DSPy prediction
        idea = prediction.story_idea if hasattr(prediction, 'story_idea') else StoryIdea(title=prediction.title, body=prediction.body)
        ideas.append(idea)
    
    # Log successful generation
    logger.log_optimization_step("idea_generation_success", {
        "genre": request.genre,
        "platform": request.platform,
        "requirements": request.requirements_section,
        "generated_ideas_count": len(ideas),
        "ideas_preview": [f"{idea.title}: {idea.body[:100]}..." if len(idea.body) > 100 else f"{idea.title}: {idea.body}" for idea in ideas[:2]]
    }, "generation")
    
    return ideas

def run_flat_optimization(auto_mode: str = "medium") -> Tuple[dspy.Module, List[dspy.Example]]:
    """Run flat (single-group) optimization - current approach"""
    print(f"ğŸš€ å¼€å§‹å¹³é¢ä¼˜åŒ– (æ¨¡å¼: {auto_mode}) - æ‰€æœ‰æŒ‡æ ‡ç»Ÿä¸€ä¼˜åŒ–")
    print("=" * 60)
    
    try:
        # Log optimization start
        logger.log_optimization_step("flat_optimization_start", {
            "auto_mode": auto_mode,
            "max_bootstrapped_demos": 4,
            "num_threads": 8,
            "max_labeled_demos": 4,
            "seed": 42
        }, "optimization_start")
        
        # Create training examples
        train_examples = create_training_examples()
        print(f"åˆ›å»ºäº† {len(train_examples)} ä¸ªè®­ç»ƒæ ·ä¾‹")
        
        # Log training data
        logger.log_training_data(train_examples, "flat_optimization_training")
        golden_examples = load_golden_examples()
        if golden_examples:
            logger.log_golden_examples(golden_examples)
        
        # Create evaluator and metric (single overall metric)
        evaluator = StoryIdeaEvaluator()
        metric = create_evaluation_metric(evaluator)  # This uses the overall score
        
        logger.log_optimization_step("metric_creation", {
            "evaluator_type": "StoryIdeaEvaluator",
            "metric_type": "single_overall_metric"
        }, "configuration")
        
        # Configure MIPROv2 optimizer
        optimizer = MIPROv2(
            metric=metric,
            auto=auto_mode,
            max_bootstrapped_demos=4,
            num_threads=8,
            max_labeled_demos=4,
            verbose=True,
            track_stats=True,
            seed=42
        )
        
        logger.log_optimization_step("optimizer_configuration", {
            "optimizer_type": "MIPROv2",
            "auto_mode": auto_mode,
            "training_examples_count": len(train_examples)
        }, "configuration")
        
        print(f"é…ç½®å¹³é¢ä¼˜åŒ–å™¨å®Œæˆï¼Œå¼€å§‹è®­ç»ƒ...")
        
        # Compile the module
        base_module = BrainstormModule()
        logger.log_optimization_step("module_compilation_start", {
            "base_module_type": "BrainstormModule"
        }, "compilation")
        
        compiled_module = optimizer.compile(
            base_module, 
            trainset=train_examples,
            requires_permission_to_run=False
        )
        
        logger.log_optimization_step("module_compilation_complete", {
            "compiled_module_type": str(type(compiled_module)),
            "success": True
        }, "compilation")
        
        # Log model prompts
        logger.log_model_prompts(compiled_module, "flat_optimized_model")
        
        print(f"âœ… å¹³é¢ä¼˜åŒ–å®Œæˆ!")
        return compiled_module, train_examples
        
    except Exception as e:
        logger.log_error(e, "flat_optimization")
        print(f"âŒ å¹³é¢ä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("åœæ­¢æ‰§è¡Œ")
        sys.exit(1)

def run_grouped_optimization(auto_mode: str = "medium") -> Tuple[Dict[str, dspy.Module], List[dspy.Example]]:
    """Run grouped optimization - separate optimization for different evaluation aspects"""
    print(f"ğŸš€ å¼€å§‹åˆ†ç»„ä¼˜åŒ– (æ¨¡å¼: {auto_mode}) - åˆ†åˆ«ä¼˜åŒ–ä¸åŒè¯„ä¼°ç»´åº¦")
    print("=" * 60)
    
    try:
        # Create evaluator and grouped metrics
        evaluator = StoryIdeaEvaluator()
        grouped_metrics = create_grouped_evaluation_metrics(evaluator, use_single_group=False)
        
        optimized_modules = {}
        all_training_examples = []
        
        for group_name, metric in grouped_metrics.items():
            print(f"\nğŸ“Š ä¼˜åŒ–ç»„åˆ«: {group_name}")
            print("-" * 40)
            
            # Create group-specific or shared training examples
            if group_name in ["creativity", "feasibility", "content_quality"]:
                train_examples = create_group_specific_training_examples(group_name)
                print(f"  ä½¿ç”¨ {len(train_examples)} ä¸ªé’ˆå¯¹æ€§è®­ç»ƒæ ·ä¾‹")
            else:
                train_examples = create_training_examples()
                print(f"  ä½¿ç”¨ {len(train_examples)} ä¸ªé€šç”¨è®­ç»ƒæ ·ä¾‹")
            
            all_training_examples.extend(train_examples)
            
            # Configure optimizer for this group
            optimizer = MIPROv2(
                metric=metric,
                auto=auto_mode,
                max_bootstrapped_demos=3,  # Slightly fewer demos per group
                num_threads=6,
                max_labeled_demos=3,
                verbose=True,
                track_stats=True,
                seed=42 + hash(group_name) % 100  # Different seed per group
            )
            
            print(f"  å¼€å§‹ä¼˜åŒ– {group_name} ç»„...")
            
            # Compile module for this group
            base_module = BrainstormModule()
            compiled_module = optimizer.compile(
                base_module,
                trainset=train_examples,
                requires_permission_to_run=False
            )
            
            optimized_modules[group_name] = compiled_module
            print(f"  âœ… {group_name} ç»„ä¼˜åŒ–å®Œæˆ!")
        
        print(f"\nâœ… æ‰€æœ‰åˆ†ç»„ä¼˜åŒ–å®Œæˆ! å…±ä¼˜åŒ–äº† {len(optimized_modules)} ä¸ªç»„åˆ«")
        return optimized_modules, list(set(all_training_examples))  # Remove duplicates
        
    except Exception as e:
        print(f"âŒ åˆ†ç»„ä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("åœæ­¢æ‰§è¡Œ")
        sys.exit(1)

def evaluate_model_performance(module, test_examples: List[dspy.Example], name: str) -> Tuple[float, Dict[str, float]]:
    """Evaluate model performance on test examples, return overall score and detailed scores"""
    print(f"\nğŸ“Š è¯„ä¼° {name} æ¨¡å‹æ€§èƒ½")
    print("-" * 40)
    
    # Log evaluation start
    logger.log_optimization_step(f"evaluation_start_{name}", {
        "model_name": name,
        "test_examples_count": len(test_examples),
        "evaluation_limit": 5
    }, "evaluation")
    
    evaluator = StoryIdeaEvaluator()
    total_scores = []
    detailed_scores = {
        'novelty': [], 'feasibility': [], 'structure': [], 
        'detail': [], 'logical_coherence': [], 'genre': [], 'engagement': []
    }
    
    test_cases_results = []
    
    for i, example in enumerate(test_examples[:MAX_TEST_EXAMPLES]):
        try:
            request = BrainstormRequest(
                genre=example.genre,
                platform=example.platform,
                requirements_section=example.requirements_section
            )
            
            # Generate ideas using DSPy
            ideas = generate_ideas(module, request)
            
            # Log generated examples for this test case
            ideas_as_strings = [f"{idea.title}: {idea.body}" for idea in ideas]
            logger.log_generated_examples(ideas_as_strings, f"{name}_case_{i+1}_{example.genre}")
            
            # Evaluate
            result = evaluator.evaluate(ideas[0], request)
            total_scores.append(result.overall_score)
            
            # Collect detailed scores
            detailed_scores['novelty'].append(result.novelty_score)
            detailed_scores['feasibility'].append(result.feasibility_score)
            detailed_scores['structure'].append(result.structure_score)
            detailed_scores['detail'].append(result.detail_score)
            detailed_scores['logical_coherence'].append(result.logical_coherence_score)
            detailed_scores['genre'].append(result.genre_score)
            detailed_scores['engagement'].append(result.engagement_score)
            
            # Store test case result for logging
            test_case_result = {
                "case_number": i + 1,
                "genre": example.genre,
                "platform": example.platform,
                "requirements": example.requirements_section,
                "generated_ideas": [{"title": idea.title, "body": idea.body} for idea in ideas],
                "overall_score": result.overall_score,
                "detailed_scores": {
                    "novelty": result.novelty_score,
                    "feasibility": result.feasibility_score,
                    "structure": result.structure_score,
                    "detail": result.detail_score,
                    "logical_coherence": result.logical_coherence_score,
                    "genre": result.genre_score,
                    "engagement": result.engagement_score
                }
            }
            test_cases_results.append(test_case_result)
            
            print(f"  æ¡ˆä¾‹ {i+1} ({example.genre}): {result.overall_score:.1f}/10")
            
        except Exception as e:
            logger.log_error(e, f"evaluate_model_performance_case_{i+1}")
            print(f"  âŒ æ¡ˆä¾‹ {i+1} è¯„ä¼°å¤±è´¥: {e}")
            print("åœæ­¢æ‰§è¡Œ")
            sys.exit(1)
    
    if total_scores:
        avg_score = sum(total_scores) / len(total_scores)
        
        # Calculate average detailed scores
        avg_detailed_scores = {}
        for metric_name, scores in detailed_scores.items():
            if scores:
                avg_detailed_scores[metric_name] = sum(scores) / len(scores)
            else:
                avg_detailed_scores[metric_name] = 0.0
        
        # Log evaluation results
        evaluation_results = {
            "model_name": name,
            "overall_score": avg_score,
            "detailed_scores": avg_detailed_scores,
            "test_cases": test_cases_results,
            "total_test_cases": len(test_cases_results)
        }
        logger.log_evaluation_results(evaluation_results, f"evaluation_{name}")
        
        print(f"\n  å¹³å‡åˆ†æ•°: {avg_score:.1f}/10")
        print(f"  è¯¦ç»†åˆ†æ•°:")
        for metric_name, score in avg_detailed_scores.items():
            print(f"    {metric_name}: {score:.1f}/10")
        
        return avg_score, avg_detailed_scores
    else:
        print("  âŒ æ— æœ‰æ•ˆè¯„ä¼°ç»“æœ")
        sys.exit(1)

def evaluate_grouped_models(grouped_modules: Dict[str, dspy.Module], test_examples: List[dspy.Example]) -> Tuple[float, Dict[str, float]]:
    """Evaluate grouped models and average their scores"""
    print(f"\nğŸ“Š è¯„ä¼°åˆ†ç»„ä¼˜åŒ–æ¨¡å‹æ€§èƒ½")
    print("-" * 40)
    
    group_scores = {}
    group_detailed_scores = {}
    
    # Evaluate each group
    for group_name, module in grouped_modules.items():
        overall_score, detailed_scores = evaluate_model_performance(module, test_examples, f"åˆ†ç»„-{group_name}")
        group_scores[group_name] = overall_score
        group_detailed_scores[group_name] = detailed_scores
    
    # Calculate averaged final score
    if group_scores:
        final_avg_score = sum(group_scores.values()) / len(group_scores)
        
        # Average detailed scores across groups
        final_detailed_scores = {}
        metric_names = ['novelty', 'feasibility', 'structure', 'detail', 'logical_coherence', 'genre', 'engagement']
        
        for metric_name in metric_names:
            metric_scores = [scores.get(metric_name, 0) for scores in group_detailed_scores.values()]
            final_detailed_scores[metric_name] = sum(metric_scores) / len(metric_scores) if metric_scores else 0.0
        
        print(f"\nğŸ¯ åˆ†ç»„æ¨¡å‹æœ€ç»ˆå¹³å‡åˆ†æ•°: {final_avg_score:.1f}/10")
        print(f"  æœ€ç»ˆè¯¦ç»†åˆ†æ•°:")
        for metric_name, score in final_detailed_scores.items():
            print(f"    {metric_name}: {score:.1f}/10")
        
        return final_avg_score, final_detailed_scores
    else:
        print("  âŒ æ— æœ‰æ•ˆåˆ†ç»„è¯„ä¼°ç»“æœ")
        sys.exit(1)

def save_optimized_model(module, name: str, score: float, detailed_scores: Dict[str, float] = None, mode: str = "flat"):
    """Save optimized model with MLflow"""
    try:
        run_name = f"brainstorm_{mode}_{name}"
        with mlflow.start_run(run_name=run_name):
            # Log parameters
            mlflow.log_param("optimization_mode", mode)
            mlflow.log_param("optimizer_type", name)
            mlflow.log_param("model_type", "BrainstormModule")
            
            # Log metrics
            mlflow.log_metric("average_score", score)
            
            # Log detailed scores if available
            if detailed_scores:
                for metric_name, metric_score in detailed_scores.items():
                    mlflow.log_metric(f"avg_{metric_name}_score", metric_score)
            
            # Log model with proper input example format
            input_example = {
                "genre": "éƒ½å¸‚çˆ±æƒ…",
                "platform": "æŠ–éŸ³",
                "requirements_section": "æµªæ¼«ç”œèœœçš„çˆ±æƒ…æ•…äº‹"
            }
            
            model_info = mlflow.dspy.log_model(
                module,
                artifact_path="model",
                input_example=input_example
            )
            
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_info.model_uri}")
            return model_info
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        print("åœæ­¢æ‰§è¡Œ")
        sys.exit(1)

def show_golden_examples_summary():
    """Show a summary of loaded golden examples"""
    golden_examples = load_golden_examples()
    if not golden_examples:
        return
        
    print(f"\nğŸ“‹ é»„é‡‘æ ·ä¾‹è¯¦æƒ…:")
    print("-" * 50)
    for i, example in enumerate(golden_examples[:3], 1):  # Show first 3 examples
        print(f"  æ ·ä¾‹ {i}:")
        print(f"    ç±»å‹: {example.genre}")
        print(f"    å¹³å°: {example.platform}")
        print(f"    è¦æ±‚: {example.requirements_section}")
        if hasattr(example, 'ideas') and example.ideas:
            content_preview = example.ideas[0][:50] + "..." if len(example.ideas[0]) > 50 else example.ideas[0]
            print(f"    å†…å®¹: {content_preview}")
        print()
    
    if len(golden_examples) > 3:
        print(f"  ...è¿˜æœ‰ {len(golden_examples) - 3} ä¸ªæ ·ä¾‹")

def run_optimization():
    """Run optimization based on the configured mode"""
    print(f"ğŸ§ª æ•…äº‹åˆ›æ„ç”Ÿæˆä¼˜åŒ–ç³»ç»Ÿ - {OPTIMIZATION_MODE.upper()} æ¨¡å¼")
    print("=" * 70)
    
    # Show summary of golden examples
    show_golden_examples_summary()
    
    # Create test examples for evaluation
    test_examples = [
        dspy.Example(genre="å…ˆå©šåçˆ±", platform="æŠ–éŸ³", requirements_section="å¥‘çº¦å©šå§»ï¼Œæƒ…æ„ŸçœŸå®"),
        dspy.Example(genre="æ¶å¥³", platform="å°çº¢ä¹¦", requirements_section="æ¶æ¯’å¥³é…é€†è¢­ï¼ŒåŒé‡äººæ ¼"),
        dspy.Example(genre="æ®‹ç–¾å¤§ä½¬", platform="å¿«æ‰‹", requirements_section="æ®‹ç–¾å¤§ä½¬éšè—èº«ä»½"),
        dspy.Example(genre="åå®«", platform="æŠ–éŸ³", requirements_section="åå®«äº‰æ–—ï¼Œæƒè°‹è®¾è®¡"),
        dspy.Example(genre="å¤ä»‡", platform="å°çº¢ä¹¦", requirements_section="å¤ä»‡ä¸»é¢˜ï¼Œæƒ…èŠ‚ç´§å‡‘")
    ]
    
    if OPTIMIZATION_MODE == "flat":
        # Run flat optimization
        print("ğŸ“‹ è¿è¡Œå¹³é¢ä¼˜åŒ–æ¨¡å¼...")
        optimized_module, _ = run_flat_optimization("medium")
        
        # Evaluate the optimized model
        score, detailed_scores = evaluate_model_performance(optimized_module, test_examples, "å¹³é¢ä¼˜åŒ–æ¨¡å‹")
        
        # Inspect and save results
        print(f"\nğŸ” æ£€æŸ¥ä¼˜åŒ–ç»“æœ:")
        inspect_optimized_module(optimized_module, "å¹³é¢ä¼˜åŒ–")
        save_optimized_prompts(optimized_module, f"{OPTIMIZATION_MODE}_optimization")
        save_optimized_model(optimized_module, "miprov2_medium", score, detailed_scores, OPTIMIZATION_MODE)
        
        print(f"\nâœ… å¹³é¢ä¼˜åŒ–å®Œæˆ! æœ€ç»ˆå¾—åˆ†: {score:.1f}/10")
        
    elif OPTIMIZATION_MODE == "grouped":
        # Run grouped optimization
        print("ğŸ“‹ è¿è¡Œåˆ†ç»„ä¼˜åŒ–æ¨¡å¼...")
        grouped_modules, _ = run_grouped_optimization("medium")
        
        # Evaluate the grouped models
        final_score, final_detailed_scores = evaluate_grouped_models(grouped_modules, test_examples)
        
        # Inspect and save results for each group
        print(f"\nğŸ” æ£€æŸ¥åˆ†ç»„ä¼˜åŒ–ç»“æœ:")
        for group_name, module in grouped_modules.items():
            inspect_optimized_module(module, f"åˆ†ç»„ä¼˜åŒ–-{group_name}")
            save_optimized_prompts(module, f"{OPTIMIZATION_MODE}_optimization_{group_name}")
            
            # Save individual group models
            group_score, group_detailed = evaluate_model_performance(module, test_examples[:2], f"åˆ†ç»„-{group_name}")
            save_optimized_model(module, f"miprov2_{group_name}", group_score, group_detailed, OPTIMIZATION_MODE)
        
        print(f"\nâœ… åˆ†ç»„ä¼˜åŒ–å®Œæˆ! æœ€ç»ˆå¹³å‡å¾—åˆ†: {final_score:.1f}/10")
        
    else:
        print(f"âŒ æœªçŸ¥çš„ä¼˜åŒ–æ¨¡å¼: {OPTIMIZATION_MODE}")
        print("è¯·å°† OPTIMIZATION_MODE è®¾ç½®ä¸º 'flat' æˆ– 'grouped'")
        sys.exit(1)

def main():
    """Main optimization workflow"""
    try:
        # Log initial configuration
        config = {
            "optimization_mode": OPTIMIZATION_MODE,
            "timestamp": datetime.now().isoformat(),
            "mlflow_experiment": f"Brainstorm_{OPTIMIZATION_MODE.title()}_Optimization",
            "python_version": sys.version,
            "script_path": __file__
        }
        logger.log_configuration(config)
        
        # Setup MLflow with mode-specific experiment name
        experiment_name = f"Brainstorm_{OPTIMIZATION_MODE.title()}_Optimization"
        mlflow.set_experiment(experiment_name)
        mlflow.dspy.autolog()
        
        print(f"ğŸ“Š MLflow å®éªŒ: {experiment_name}")
        
        # Log MLflow setup
        logger.log_optimization_step("mlflow_setup", {
            "experiment_name": experiment_name,
            "autolog_enabled": True
        }, "initialization")
        
        # Run optimization
        run_optimization()
        
        # Create final summary
        summary_file = logger.create_final_summary()
        
        print(f"\nğŸ“ ä¼˜åŒ–å®Œæˆæ€»ç»“:")
        print(f"1. ä¼˜åŒ–æ¨¡å¼: {OPTIMIZATION_MODE.upper()}")
        print(f"2. ç¼“å­˜æœºåˆ¶: å¯ç”¨ (é¿å…é‡å¤è¯„ä¼°)")
        print(f"3. MLflow å®éªŒ: {experiment_name}")
        print(f"4. æç¤ºè¯æ–‡ä»¶: optimized_prompts/{OPTIMIZATION_MODE}_optimization_*.txt")
        print(f"5. è¯¦ç»†æ—¥å¿—: {logger.run_dir}")
        print(f"6. æ€»ç»“æ–‡ä»¶: {summary_file}")
        print(f"7. è¦åˆ‡æ¢æ¨¡å¼ï¼Œè¯·ä¿®æ”¹ä»£ç ä¸­çš„ OPTIMIZATION_MODE å¸¸é‡")
        
    except Exception as e:
        logger.log_error(e, "main_workflow")
        print(f"âŒ ä¸»æµç¨‹å‘ç”Ÿé”™è¯¯: {e}")
        print("ç¨‹åºç»ˆæ­¢")
        sys.exit(1)

if __name__ == "__main__":
    main()
