#!/usr/bin/env python3
"""
Script to demonstrate how to use optimized DSPy modules
Shows different ways to load and use optimized models from the optimization process
"""

import os
import json
import mlflow
import dspy
import random
import itertools
from typing import Optional, Dict, Any, List
from brainstorm_module import BrainstormModule
from common import BrainstormRequest, StoryIdea

class RequirementsVariator:
    """Generate varied requirements to avoid caching"""
    
    def __init__(self):
        # Story setting categories
        self.settings = [
            "ç°ä»£éƒ½å¸‚", "å¤è£…å®«å»·", "æ ¡å›­é’æ˜¥", "èŒåœºå•†æˆ˜", "è±ªé—¨ä¸–å®¶", 
            "åŒ»é™¢", "å¾‹å¸ˆäº‹åŠ¡æ‰€", "å¨±ä¹åœˆ", "å†›è¥", "ä¹¡æ‘ç”°å›­",
            "æµ·å¤–ç•™å­¦", "å°é•‡ç”Ÿæ´»", "ç½‘ç»œæ¸¸æˆ", "ç›´æ’­å¹³å°", "åˆ›ä¸šå…¬å¸"
        ]
        
        # Character archetypes
        self.male_leads = [
            "éœ¸é“æ€»è£", "æ¸©æ¶¦å­¦éœ¸", "å†·é…·åŒ»ç”Ÿ", "æ­£ä¹‰å¾‹å¸ˆ", "å¤©æ‰ç¨‹åºå‘˜",
            "èŒä¸šå†›äºº", "çŸ¥åå¯¼æ¼”", "äººæ°”æ­Œæ‰‹", "ä½“è‚²å¥å°†", "å¨è‰ºå¤§å¸ˆ",
            "æŠ•èµ„å¤©æ‰", "ç§‘ç ”å­¦è€…", "æ—¶å°šè®¾è®¡å¸ˆ", "æ¸¸æˆä¸»æ’­", "åˆ›ä¸šé’å¹´"
        ]
        
        self.female_leads = [
            "ç‹¬ç«‹å¥³å¼ºäºº", "è½¯èŒå°æŠ¤å£«", "å¤©æ‰è®¾è®¡å¸ˆ", "çŸ¥æ€§ç¼–è¾‘", "æ´»æ³¼è®°è€…",
            "æ¸©æŸ”æ•™å¸ˆ", "å†·é™æ³•åŒ»", "ç”œç¾ä¸»æ’­", "åšå¼ºå•äº²å¦ˆå¦ˆ", "æ–‡è‰ºä½œå®¶",
            "æ—¶å°šåšä¸»", "å¿ƒç†å’¨è¯¢å¸ˆ", "ç¾é£Ÿåšä¸»", "å¥èº«æ•™ç»ƒ", "å…¬ç›Šå¿—æ„¿è€…"
        ]
        
        # Relationship dynamics
        self.relationships = [
            "åŒå¼ºCP", "é’æ¢…ç«¹é©¬", "æ•Œäººå˜æ‹äºº", "å‡æˆçœŸåš", "å¥‘çº¦å…³ç³»",
            "æš—æ‹æˆçœŸ", "é‡é€¢æ‹äºº", "å¸ˆç”Ÿæ‹", "ä¸Šå¸ä¸‹å±", "ç«äº‰å¯¹æ‰‹",
            "ç½‘å‹è§é¢", "ç›¸äº²å¯¹è±¡", "é‚»å±…å…³ç³»", "åˆä½œä¼™ä¼´", "æ•‘å‘½æ©äºº"
        ]
        
        # Plot elements
        self.plot_elements = [
            "è¯¯ä¼šåˆ†ç¦»", "è¿½å¦»ç«è‘¬åœº", "å¤±å¿†æ¢—", "èº«ä»½äº’æ¢", "æ—¶ç©ºç©¿è¶Š",
            "è±ªé—¨äº‰äº§", "èŒåœºå‡çº§", "åˆ›ä¸šå¥‹æ–—", "åŒ»ç–—æ•‘æ²»", "æ³•åº­è¾©æŠ¤",
            "å¨±ä¹åœˆé£æ³¢", "ç½‘ç»œå±æœº", "å®¶æ—ç§˜å¯†", "å›½é™…ç«èµ›", "å…¬ç›Šæ•‘åŠ©"
        ]
        
        # Emotional tones
        self.tones = [
            "ç”œå® æ—¥å¸¸", "è™æ‹æƒ…æ·±", "æç¬‘è½»æ¾", "æ²»æ„ˆæ¸©æš–", "åŠ±å¿—å‘ä¸Š",
            "æ‚¬ç–‘åˆºæ¿€", "æµªæ¼«å”¯ç¾", "ç°å®å‘", "çˆ½æ–‡èŠ‚å¥", "ç»†æ°´é•¿æµ"
        ]
        
        # Special requirements
        self.special_tags = [
            "å»è„¸è°±åŒ–", "åå¥—è·¯", "ç°å®ä¸»ä¹‰", "åŒå‘å¥”èµ´", "æˆé•¿å‘",
            "ç¾¤åƒå‰§", "å¤šçº¿å™äº‹", "æ—¶é—´è·¨åº¦å¤§", "å›½é™…èƒŒæ™¯", "ç§‘æŠ€å…ƒç´ ",
            "ç¯ä¿ä¸»é¢˜", "ç¤¾ä¼šè®®é¢˜", "ä»£é™…æ²Ÿé€š", "æ–‡åŒ–ä¼ æ‰¿", "åˆ›æ–°é¢˜æ"
        ]
        
        # Generate all possible combinations for uniqueness tracking
        self._used_combinations = set()
        self.combination_pool = self._generate_combination_pool()
        random.shuffle(self.combination_pool)
        self.current_index = 0
        
    def _generate_combination_pool(self) -> List[str]:
        """Generate a large pool of unique requirement combinations"""
        combinations = []
        
        # Generate different combination patterns
        patterns = [
            # Pattern 1: Setting + Male Lead + Female Lead + Relationship
            lambda: f"{random.choice(self.settings)}, {random.choice(self.male_leads)}x{random.choice(self.female_leads)}, {random.choice(self.relationships)}",
            
            # Pattern 2: Setting + Relationship + Plot Element + Tone
            lambda: f"{random.choice(self.settings)}, {random.choice(self.relationships)}, {random.choice(self.plot_elements)}, {random.choice(self.tones)}",
            
            # Pattern 3: Character Types + Plot Elements + Special Tags
            lambda: f"{random.choice(self.male_leads)}x{random.choice(self.female_leads)}, {random.choice(self.plot_elements)}, {random.choice(self.special_tags)}",
            
            # Pattern 4: Setting + Multiple Plot Elements
            lambda: f"{random.choice(self.settings)}, {random.choice(self.plot_elements)}, {random.choice(self.plot_elements)}, {random.choice(self.tones)}",
            
            # Pattern 5: Comprehensive combination
            lambda: f"{random.choice(self.settings)}, {random.choice(self.male_leads)}x{random.choice(self.female_leads)}, {random.choice(self.relationships)}, {random.choice(self.plot_elements)}, {random.choice(self.tones)}, {random.choice(self.special_tags)}",
        ]
        
        # Generate combinations using different patterns
        for _ in range(1000):  # Generate 1000 unique combinations
            pattern = random.choice(patterns)
            combo = pattern()
            if combo not in combinations:
                combinations.append(combo)
                
        return combinations
    
    def get_unique_requirements(self) -> str:
        """Get a unique requirements string, cycling through the pool"""
        if self.current_index >= len(self.combination_pool):
            # Reshuffle and reset if we've used all combinations
            random.shuffle(self.combination_pool)
            self.current_index = 0
            print("ğŸ”„ é‡æ–°æ‰“ä¹±éœ€æ±‚ç»„åˆæ± ")
            
        requirements = self.combination_pool[self.current_index]
        self.current_index += 1
        return requirements
    
    def get_unique_request(self, genre: str, platform: str) -> BrainstormRequest:
        """Generate a BrainstormRequest with unique requirements"""
        return BrainstormRequest(
            genre=genre,
            platform=platform,
            requirements_section=self.get_unique_requirements()
        )
    
    def get_batch_requests(self, genre: str, platform: str, count: int) -> List[BrainstormRequest]:
        """Generate a batch of unique requests"""
        return [self.get_unique_request(genre, platform) for _ in range(count)]
    
    def get_stats(self) -> Dict[str, int]:
        """Get statistics about the variator"""
        return {
            "total_combinations": len(self.combination_pool),
            "used_combinations": self.current_index,
            "remaining_combinations": len(self.combination_pool) - self.current_index
        }

# Global variator instance
requirements_variator = RequirementsVariator()

def extract_prompts_from_module(module: dspy.Module, output_file: str = "extracted_prompts.json") -> bool:
    """Extract system and user prompts from a DSPy module and save to file for TypeScript usage"""
    print(f"ğŸ” æå–æ¨¡å—æç¤ºè¯åˆ°æ–‡ä»¶: {output_file}")
    
    try:
        prompts_data = {
            "extracted_at": None,
            "module_type": type(module).__name__,
            "prompts": {},
            "demos": [],
            "metadata": {}
        }
        
        import datetime
        prompts_data["extracted_at"] = datetime.datetime.now().isoformat()
        
        # Extract prompts from the main predictor
        if hasattr(module, 'generate_idea'):
            predictor = module.generate_idea
            
            # Try to get the signature
            if hasattr(predictor, 'signature'):
                signature = predictor.signature
                prompts_data["metadata"]["signature_instructions"] = getattr(signature, 'instructions', '')
                prompts_data["metadata"]["input_fields"] = list(signature.input_fields.keys()) if hasattr(signature, 'input_fields') else []
                prompts_data["metadata"]["output_fields"] = list(signature.output_fields.keys()) if hasattr(signature, 'output_fields') else []
            
            # Try to extract the actual prompt templates used
            # This varies depending on DSPy version and LM type
            if hasattr(predictor, 'lm') and predictor.lm:
                lm = predictor.lm
                
                # Get the last request if available (from DSPy's request history)
                if hasattr(lm, 'history') and lm.history:
                    last_request = lm.history[-1]
                    if 'prompt' in last_request:
                        prompts_data["prompts"]["last_full_prompt"] = last_request['prompt']
                    if 'messages' in last_request:
                        prompts_data["prompts"]["last_messages"] = last_request['messages']
            
            # Try to construct a sample prompt by calling the predictor's _generate method
            try:
                # Create a sample input
                sample_input = {
                    'genre': 'ç”œå® ',
                    'platform': 'æŠ–éŸ³',  
                    'requirements_section': 'ç°ä»£éƒ½å¸‚, éœ¸é“æ€»è£xç‹¬ç«‹å¥³å¼ºäºº, åŒå¼ºCP'
                }
                
                # Get the formatted prompt (this might trigger LM call, so we'll catch any errors)
                if hasattr(predictor, '_generate'):
                    # Try to intercept the prompt before it goes to LM
                    original_lm = predictor.lm
                    
                    class MockLM:
                        def __init__(self):
                            self.captured_prompt = None
                            self.captured_messages = None
                            
                        def __call__(self, prompt=None, messages=None, **kwargs):
                            self.captured_prompt = prompt
                            self.captured_messages = messages
                            # Return a mock response to avoid actual LM call
                            return ["title: ç¤ºä¾‹æ ‡é¢˜\nbody: ç¤ºä¾‹å†…å®¹"]
                        
                        def generate(self, prompt=None, messages=None, **kwargs):
                            return self.__call__(prompt, messages, **kwargs)
                    
                    mock_lm = MockLM()
                    predictor.lm = mock_lm
                    
                    try:
                        # This should capture the prompt without making real LM call
                        predictor(**sample_input)
                        
                        if mock_lm.captured_prompt:
                            prompts_data["prompts"]["system_user_prompt"] = mock_lm.captured_prompt
                        if mock_lm.captured_messages:
                            prompts_data["prompts"]["messages_format"] = mock_lm.captured_messages
                            
                    except Exception as e:
                        print(f"âš ï¸ æ¨¡æ‹Ÿè°ƒç”¨å¤±è´¥: {e}")
                    finally:
                        # Restore original LM
                        predictor.lm = original_lm
                        
            except Exception as e:
                print(f"âš ï¸ æç¤ºè¯æ„å»ºå¤±è´¥: {e}")
            
            # Extract demonstrations/examples
            if hasattr(predictor, 'demos') and predictor.demos:
                print(f"ğŸ“š å‘ç° {len(predictor.demos)} ä¸ªæ¼”ç¤ºç¤ºä¾‹")
                for i, demo in enumerate(predictor.demos):
                    demo_data = {}
                    
                    # Extract input fields
                    for field in ['genre', 'platform', 'requirements_section']:
                        if hasattr(demo, field):
                            demo_data[field] = getattr(demo, field)
                    
                    # Extract output fields  
                    for field in ['title', 'body']:
                        if hasattr(demo, field):
                            demo_data[field] = getattr(demo, field)
                    
                    prompts_data["demos"].append({
                        "demo_index": i,
                        "data": demo_data
                    })
        
        # Try to extract template/instruction patterns
        try:
            # Look for common DSPy prompt patterns
            if hasattr(module, 'generate_idea') and hasattr(module.generate_idea, 'signature'):
                sig = module.generate_idea.signature
                
                # Build a template based on signature
                template_parts = []
                
                if hasattr(sig, 'instructions') and sig.instructions:
                    template_parts.append(f"Instructions: {sig.instructions}")
                
                # Add input format
                if hasattr(sig, 'input_fields'):
                    input_template = "Input Format:\n"
                    for name, field in sig.input_fields.items():
                        desc = getattr(field, 'desc', '') or name
                        input_template += f"- {name}: {desc}\n"
                    template_parts.append(input_template)
                
                # Add output format
                if hasattr(sig, 'output_fields'):
                    output_template = "Output Format:\n"
                    for name, field in sig.output_fields.items():
                        desc = getattr(field, 'desc', '') or name
                        output_template += f"- {name}: {desc}\n"
                    template_parts.append(output_template)
                
                if template_parts:
                    prompts_data["prompts"]["template_structure"] = "\n\n".join(template_parts)
                    
        except Exception as e:
            print(f"âš ï¸ æ¨¡æ¿æå–å¤±è´¥: {e}")
        
        # Write to file
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(prompts_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æç¤ºè¯å·²ä¿å­˜åˆ°: {output_file}")
        print(f"   - æ¨¡å—ç±»å‹: {prompts_data['module_type']}")
        print(f"   - æ¼”ç¤ºç¤ºä¾‹æ•°: {len(prompts_data['demos'])}")
        print(f"   - æå–çš„æç¤ºè¯ç±»å‹: {list(prompts_data['prompts'].keys())}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æç¤ºè¯æå–å¤±è´¥: {e}")
        return False

def setup_environment():
    """Setup LLM and MLflow environment"""
    print("ğŸ”§ è®¾ç½®ç¯å¢ƒ...")
    
    # LLM is automatically configured when importing common.py
    # Set MLflow tracking URI (optional)
    # mlflow.set_tracking_uri("sqlite:///mlflow.db")
    
    print("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ")

def load_optimized_from_mlflow(experiment_name: str = "Brainstorm_Flat_deepseek-chat_Optimization") -> Optional[dspy.Module]:
    """Load optimized module from MLflow"""
    print(f"ğŸ“¦ ä» MLflow åŠ è½½ä¼˜åŒ–æ¨¡å‹ (å®éªŒ: {experiment_name})")
    
    try:
        # Set the experiment
        experiment = mlflow.get_experiment_by_name(experiment_name)
        if not experiment:
            print(f"âŒ å®éªŒ '{experiment_name}' ä¸å­˜åœ¨")
            return None
        
        # Get the latest run
        runs = mlflow.search_runs(
            experiment_ids=[experiment.experiment_id],
            order_by=["start_time DESC"],
            max_results=1
        )
        
        if runs.empty:
            print(f"âŒ å®éªŒ '{experiment_name}' ä¸­æ²¡æœ‰è¿è¡Œè®°å½•")
            return None
        
        run_id = runs.iloc[0].run_id
        print(f"ğŸ“‹ æ‰¾åˆ°æœ€æ–°è¿è¡Œ: {run_id}")
        
        # Load the model
        model_uri = f"runs:/{run_id}/model"
        loaded_model = mlflow.dspy.load_model(model_uri)
        
        print(f"âœ… æˆåŠŸä» MLflow åŠ è½½ä¼˜åŒ–æ¨¡å‹")
        return loaded_model
        
    except Exception as e:
        print(f"âŒ ä» MLflow åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return None

def load_optimized_from_prompts(prompts_file: str = "optimized_prompts/flat_optimization_optimized_prompts.json") -> Optional[dspy.Module]:
    """Load optimized module by recreating it from saved prompt information"""
    print(f"ğŸ“„ ä»æç¤ºè¯æ–‡ä»¶é‡å»ºä¼˜åŒ–æ¨¡å‹: {prompts_file}")
    
    try:
        if not os.path.exists(prompts_file):
            print(f"âŒ æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {prompts_file}")
            return None
        
        # Load the prompt information
        with open(prompts_file, 'r', encoding='utf-8') as f:
            prompt_info = json.load(f)
        
        # Create a new module
        module = BrainstormModule()
        
        # If the optimized module has demonstrations, add them
        if 'predictor' in prompt_info and 'demos' in prompt_info['predictor']:
            demos_data = prompt_info['predictor']['demos']
            print(f"ğŸ“š å‘ç° {len(demos_data)} ä¸ªç¤ºä¾‹æ¼”ç¤º")
            
            # Convert demo data to DSPy examples
            demos = []
            for demo_data in demos_data:
                # Create a DSPy example from the demo data
                if 'title' in demo_data and 'body' in demo_data:
                    demo = dspy.Example(
                        genre=demo_data.get('genre', ''),
                        platform=demo_data.get('platform', ''),
                        requirements_section=demo_data.get('requirements_section', ''),
                        title=demo_data['title'],
                        body=demo_data['body']
                    ).with_inputs('genre', 'platform', 'requirements_section')
                    demos.append(demo)
            
            # Set the demonstrations on the predictor
            if demos:
                module.generate_idea.demos = demos
                print(f"âœ… è®¾ç½®äº† {len(demos)} ä¸ªç¤ºä¾‹æ¼”ç¤º")
        
        print(f"âœ… æˆåŠŸä»æç¤ºè¯æ–‡ä»¶é‡å»ºä¼˜åŒ–æ¨¡å‹")
        return module
        
    except Exception as e:
        print(f"âŒ ä»æç¤ºè¯æ–‡ä»¶é‡å»ºæ¨¡å‹å¤±è´¥: {e}")
        return None

def compare_models(baseline_module: dspy.Module, optimized_module: dspy.Module, test_requests: list, max_content_length: int = None):
    """Compare baseline vs optimized model performance"""
    print(f"\nğŸ†š æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    for i, request in enumerate(test_requests, 1):
        print(f"\næµ‹è¯•æ¡ˆä¾‹ {i}: {request.genre} - {request.platform}")
        print(f"è¦æ±‚: {request.requirements_section}")
        print("-" * 40)
        
        # Generate with baseline
        # print("ğŸ”¸ åŸºç¡€æ¨¡å‹:")
        # try:
        #     baseline_result = baseline_module(
        #         genre=request.genre,
        #         platform=request.platform,
        #         requirements_section=request.requirements_section
        #     )
        #     baseline_idea = baseline_result.story_idea if hasattr(baseline_result, 'story_idea') else StoryIdea(title=baseline_result.title, body=baseline_result.body)
        #     print(f"  æ ‡é¢˜: {baseline_idea.title}")
            
        #     # Show full content or truncated based on max_content_length
        #     if max_content_length and len(baseline_idea.body) > max_content_length:
        #         print(f"  å†…å®¹: {baseline_idea.body[:max_content_length]}...")
        #         print(f"  (å†…å®¹é•¿åº¦: {len(baseline_idea.body)} å­—ç¬¦ï¼Œå·²æˆªå–å‰ {max_content_length} å­—ç¬¦)")
        #     else:
        #         print(f"  å†…å®¹: {baseline_idea.body}")
        #         print(f"  (å†…å®¹é•¿åº¦: {len(baseline_idea.body)} å­—ç¬¦)")
        # except Exception as e:
        #     print(f"  âŒ ç”Ÿæˆå¤±è´¥: {e}")
        
        # Generate with optimized
        print("\nğŸ”¹ ä¼˜åŒ–æ¨¡å‹:")
        try:
            optimized_result = optimized_module(
                genre=request.genre,
                platform=request.platform,
                requirements_section=request.requirements_section
            )
            optimized_idea = optimized_result.story_idea if hasattr(optimized_result, 'story_idea') else StoryIdea(title=optimized_result.title, body=optimized_result.body)
            print(f"  æ ‡é¢˜: {optimized_idea.title}")
            
            # Show full content or truncated based on max_content_length
            if max_content_length and len(optimized_idea.body) > max_content_length:
                print(f"  å†…å®¹: {optimized_idea.body[:max_content_length]}...")
                print(f"  (å†…å®¹é•¿åº¦: {len(optimized_idea.body)} å­—ç¬¦ï¼Œå·²æˆªå–å‰ {max_content_length} å­—ç¬¦)")
            else:
                print(f"  å†…å®¹: {optimized_idea.body}")
                print(f"  (å†…å®¹é•¿åº¦: {len(optimized_idea.body)} å­—ç¬¦)")
        except Exception as e:
            print(f"  âŒ ç”Ÿæˆå¤±è´¥: {e}")

def demonstrate_usage():
    """Demonstrate different ways to use optimized modules"""
    print("ğŸš€ ä¼˜åŒ–æ¨¡å—ä½¿ç”¨æ¼”ç¤º")
    print("=" * 60)
    
    # Setup environment
    setup_environment()
    
    # Create varied test requests using the requirements variator
    print(f"ğŸ“‹ ç”Ÿæˆå˜åŒ–çš„æµ‹è¯•è¯·æ±‚...")
    stats = requirements_variator.get_stats()
    print(f"   å¯ç”¨ç»„åˆæ€»æ•°: {stats['total_combinations']}")
    print(f"   å·²ä½¿ç”¨ç»„åˆ: {stats['used_combinations']}")
    
    test_requests = [
        requirements_variator.get_unique_request("ç”œå® ", "æŠ–éŸ³"),
        requirements_variator.get_unique_request("å¤ä»‡", "å¿«æ‰‹"),
        requirements_variator.get_unique_request("è™æ‹", "å°çº¢ä¹¦"),
    ]
    
    # Show the unique requirements generated
    print("\nğŸ“ ç”Ÿæˆçš„ç‹¬ç‰¹éœ€æ±‚:")
    for i, req in enumerate(test_requests, 1):
        print(f"  {i}. {req.genre} ({req.platform}): {req.requirements_section}")
    
    # Create baseline module for comparison
    baseline_module = BrainstormModule()
    print("\nâœ… åˆ›å»ºåŸºç¡€æ¨¡å‹")
    
    # Method 1: Load from MLflow (recommended for production)
    print(f"\nğŸ“¦ æ–¹æ³•1: ä» MLflow åŠ è½½ä¼˜åŒ–æ¨¡å‹")
    mlflow_model = load_optimized_from_mlflow()
    
    if mlflow_model:
        print("âœ… æˆåŠŸåŠ è½½ MLflow æ¨¡å‹ï¼Œå¼€å§‹å¯¹æ¯”æµ‹è¯•...")
        compare_models(baseline_module, mlflow_model, test_requests[:1])
    else:
        print("âš ï¸  MLflow æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡æ­¤æ–¹æ³•")
    
    # Method 2: Load from prompts file (fallback method)
    print(f"\nğŸ“„ æ–¹æ³•2: ä»æç¤ºè¯æ–‡ä»¶é‡å»ºä¼˜åŒ–æ¨¡å‹")
    prompts_model = load_optimized_from_prompts()
    
    if prompts_model:
        print("âœ… æˆåŠŸé‡å»ºä¼˜åŒ–æ¨¡å‹ï¼Œå¼€å§‹å¯¹æ¯”æµ‹è¯•...")
        compare_models(baseline_module, prompts_model, test_requests[1:])
    else:
        print("âš ï¸  æç¤ºè¯æ–‡ä»¶é‡å»ºå¤±è´¥ï¼Œè·³è¿‡æ­¤æ–¹æ³•")
    
    # Method 3: Direct usage example with unique request
    print(f"\nğŸ’¡ æ–¹æ³•3: ç›´æ¥ä½¿ç”¨ä¼˜åŒ–æ¨¡å‹ç¤ºä¾‹")
    active_model = mlflow_model or prompts_model or baseline_module
    model_type = "ä¼˜åŒ–æ¨¡å‹" if (mlflow_model or prompts_model) else "åŸºç¡€æ¨¡å‹"
    
    print(f"ä½¿ç”¨ {model_type} ç”Ÿæˆåˆ›æ„...")
    request = requirements_variator.get_unique_request("ç„å¹»", "æŠ–éŸ³")
    print(f"ç‹¬ç‰¹éœ€æ±‚: {request.requirements_section}")
    
    try:
        result = active_model(
            genre=request.genre,
            platform=request.platform,
            requirements_section=request.requirements_section
        )
        idea = result.story_idea if hasattr(result, 'story_idea') else StoryIdea(title=result.title, body=result.body)
        
        print(f"âœ… ç”ŸæˆæˆåŠŸ:")
        print(f"  æ ‡é¢˜: {idea.title}")
        print(f"  å†…å®¹: {idea.body}")
        print(f"  (å†…å®¹é•¿åº¦: {len(idea.body)} å­—ç¬¦)")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")

def production_usage_example():
    """Show how to use optimized models in production code"""
    print(f"\nğŸ­ ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    
    # This is how you would typically use optimized models in your application
    class OptimizedBrainstormService:
        def __init__(self):
            self.model = None
            self.variator = RequirementsVariator()
            self._load_best_model()
        
        def _load_best_model(self):
            """Load the best available optimized model"""
            # Try MLflow first
            self.model = load_optimized_from_mlflow()
            
            # Fallback to prompts file
            if not self.model:
                self.model = load_optimized_from_prompts()
            
            # Ultimate fallback to baseline
            if not self.model:
                print("âš ï¸ ä½¿ç”¨åŸºç¡€æ¨¡å‹ä½œä¸ºåå¤‡")
                self.model = BrainstormModule()
        
        def generate_ideas(self, genre: str, platform: str, base_requirements: str = "") -> StoryIdea:
            """Generate story ideas using the optimized model with varied requirements"""
            if not self.model:
                raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
            
            # Add variation to requirements to avoid caching
            if base_requirements:
                unique_requirements = f"{base_requirements}, {self.variator.get_unique_requirements()}"
            else:
                unique_requirements = self.variator.get_unique_requirements()
            
            result = self.model(
                genre=genre,
                platform=platform,
                requirements_section=unique_requirements
            )
            
            return result.story_idea if hasattr(result, 'story_idea') else StoryIdea(title=result.title, body=result.body)
        
        def generate_batch_ideas(self, genre: str, platform: str, count: int, base_requirements: str = "") -> List[StoryIdea]:
            """Generate multiple unique ideas"""
            ideas = []
            for i in range(count):
                try:
                    idea = self.generate_ideas(genre, platform, base_requirements)
                    ideas.append(idea)
                    print(f"  âœ… ç”Ÿæˆåˆ›æ„ {i+1}/{count}: {idea.title}")
                except Exception as e:
                    print(f"  âŒ ç”Ÿæˆåˆ›æ„ {i+1}/{count} å¤±è´¥: {e}")
            return ideas
    
    # Example usage
    try:
        service = OptimizedBrainstormService()
        
        # Generate single idea
        idea = service.generate_ideas(
            genre="ç„å¹»",
            platform="æŠ–éŸ³",
            base_requirements="ä¿®ä»™ä¸–ç•Œ, å¤©æ‰å°‘å¹´"
        )
        
        print(f"ğŸ¯ å•ä¸ªåˆ›æ„ç”Ÿæˆç»“æœ:")
        print(f"  æ ‡é¢˜: {idea.title}")
        print(f"  å†…å®¹: {idea.body}")
        print(f"  (å†…å®¹é•¿åº¦: {len(idea.body)} å­—ç¬¦)")
        
        # Generate batch ideas
        print(f"\nğŸ¯ æ‰¹é‡åˆ›æ„ç”Ÿæˆ (3ä¸ª):")
        ideas = service.generate_batch_ideas(
            genre="éƒ½å¸‚",
            platform="å¿«æ‰‹", 
            count=3,
            base_requirements="èŒåœº"
        )
        
        # Show variator stats
        stats = service.variator.get_stats()
        print(f"\nğŸ“Š éœ€æ±‚å˜åŒ–å™¨ç»Ÿè®¡:")
        print(f"  æ€»ç»„åˆæ•°: {stats['total_combinations']}")
        print(f"  å·²ä½¿ç”¨: {stats['used_combinations']}")
        print(f"  å‰©ä½™: {stats['remaining_combinations']}")
        
    except Exception as e:
        print(f"âŒ ç”Ÿäº§ç¯å¢ƒç¤ºä¾‹å¤±è´¥: {e}")

def test_cache_avoidance():
    """Test that the requirements variator successfully avoids caching"""
    print(f"\nğŸ§ª ç¼“å­˜è§„é¿æµ‹è¯•")
    print("=" * 60)
    
    module = BrainstormModule()
    
    print("ç”Ÿæˆ5ä¸ªç›¸åŒé¢˜æçš„è¯·æ±‚ï¼Œè§‚å¯Ÿæ˜¯å¦é¿å…äº†ç¼“å­˜...")
    
    requests = requirements_variator.get_batch_requests("ç”œå® ", "æŠ–éŸ³", 5)
    
    for i, request in enumerate(requests, 1):
        print(f"\nè¯·æ±‚ {i}:")
        print(f"  éœ€æ±‚: {request.requirements_section}")
        
        try:
            import time
            start_time = time.time()
            
            result = module(
                genre=request.genre,
                platform=request.platform,
                requirements_section=request.requirements_section
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            idea = result.story_idea if hasattr(result, 'story_idea') else StoryIdea(title=result.title, body=result.body)
            
            print(f"  ç”¨æ—¶: {duration:.2f}ç§’")
            print(f"  ç»“æœ: ã€{idea.title}ã€‘{idea.body}")
            print(f"  (å†…å®¹é•¿åº¦: {len(idea.body)} å­—ç¬¦)")
            
            if duration < 0.1:
                print("  âš ï¸  å¯èƒ½ä½¿ç”¨äº†ç¼“å­˜ (å“åº”è¿‡å¿«)")
            else:
                print("  âœ… å¯èƒ½é¿å…äº†ç¼“å­˜ (æ­£å¸¸LLMå“åº”æ—¶é—´)")
                
        except Exception as e:
            print(f"  âŒ ç”Ÿæˆå¤±è´¥: {e}")

def extract_all_prompts():
    """Extract prompts from all available modules for TypeScript usage"""
    print(f"\nğŸ“¤ æç¤ºè¯æå–æ¨¡å¼")
    print("=" * 60)
    
    setup_environment()
    
    # Extract from baseline module
    print("ğŸ”¸ æå–åŸºç¡€æ¨¡å‹æç¤ºè¯...")
    baseline_module = BrainstormModule()
    baseline_success = extract_prompts_from_module(baseline_module, "baseline_prompts.json")
    
    # Extract from optimized modules
    print("\nğŸ”¹ æå–ä¼˜åŒ–æ¨¡å‹æç¤ºè¯...")
    
    # Try MLflow first
    mlflow_model = load_optimized_from_mlflow()
    if mlflow_model:
        mlflow_success = extract_prompts_from_module(mlflow_model, "optimized_mlflow_prompts.json")
    else:
        mlflow_success = False
        print("âš ï¸ MLflow æ¨¡å‹ä¸å¯ç”¨")
    
    # Try prompts file method
    prompts_model = load_optimized_from_prompts()
    if prompts_model:
        prompts_success = extract_prompts_from_module(prompts_model, "optimized_prompts_file_prompts.json")
    else:
        prompts_success = False
        print("âš ï¸ æç¤ºè¯æ–‡ä»¶æ¨¡å‹ä¸å¯ç”¨")
    
    # Create a combined TypeScript-friendly export
    print("\nğŸ¯ åˆ›å»ºTypeScriptå‹å¥½çš„åˆå¹¶æ–‡ä»¶...")
    try:
        combined_data = {
            "extraction_info": {
                "extracted_at": None,
                "baseline_available": baseline_success,
                "mlflow_optimized_available": mlflow_success,
                "prompts_file_optimized_available": prompts_success
            },
            "baseline": {},
            "optimized_mlflow": {},
            "optimized_prompts_file": {}
        }
        
        import datetime
        combined_data["extraction_info"]["extracted_at"] = datetime.datetime.now().isoformat()
        
        # Load each file if it exists
        files_to_load = [
            ("baseline_prompts.json", "baseline"),
            ("optimized_mlflow_prompts.json", "optimized_mlflow"), 
            ("optimized_prompts_file_prompts.json", "optimized_prompts_file")
        ]
        
        for filename, key in files_to_load:
            if os.path.exists(filename):
                try:
                    with open(filename, 'r', encoding='utf-8') as f:
                        combined_data[key] = json.load(f)
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•åŠ è½½ {filename}: {e}")
                    combined_data[key] = {"error": str(e)}
        
        # Write combined file
        with open("all_prompts_for_typescript.json", 'w', encoding='utf-8') as f:
            json.dump(combined_data, f, ensure_ascii=False, indent=2)
        
        print("âœ… TypeScriptå‹å¥½çš„åˆå¹¶æ–‡ä»¶å·²åˆ›å»º: all_prompts_for_typescript.json")
        
        # Create a TypeScript interface file
        ts_interface = '''// Auto-generated TypeScript interfaces for extracted DSPy prompts
// Generated at: ''' + datetime.datetime.now().isoformat() + '''

export interface ExtractedPrompt {
  system_user_prompt?: string;
  messages_format?: Array<{role: string, content: string}>;
  last_full_prompt?: string;
  last_messages?: Array<{role: string, content: string}>;
  template_structure?: string;
}

export interface DemoData {
  genre?: string;
  platform?: string;
  requirements_section?: string;
  title?: string;
  body?: string;
}

export interface ExtractedDemo {
  demo_index: number;
  data: DemoData;
}

export interface ModuleMetadata {
  signature_instructions?: string;
  input_fields?: string[];
  output_fields?: string[];
}

export interface ExtractedModuleData {
  extracted_at?: string;
  module_type?: string;
  prompts: ExtractedPrompt;
  demos: ExtractedDemo[];
  metadata: ModuleMetadata;
  error?: string;
}

export interface AllPromptsData {
  extraction_info: {
    extracted_at: string;
    baseline_available: boolean;
    mlflow_optimized_available: boolean;
    prompts_file_optimized_available: boolean;
  };
  baseline: ExtractedModuleData;
  optimized_mlflow: ExtractedModuleData;
  optimized_prompts_file: ExtractedModuleData;
}

// Usage example:
// import promptsData from './all_prompts_for_typescript.json';
// const data: AllPromptsData = promptsData;
// 
// // Get the best available optimized prompt
// const getBestPrompt = (): ExtractedPrompt => {
//   if (data.extraction_info.mlflow_optimized_available && data.optimized_mlflow.prompts) {
//     return data.optimized_mlflow.prompts;
//   }
//   if (data.extraction_info.prompts_file_optimized_available && data.optimized_prompts_file.prompts) {
//     return data.optimized_prompts_file.prompts;
//   }
//   return data.baseline.prompts;
// };
'''
        
        with open("prompts-types.ts", 'w', encoding='utf-8') as f:
            f.write(ts_interface)
        
        print("âœ… TypeScriptæ¥å£æ–‡ä»¶å·²åˆ›å»º: prompts-types.ts")
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºåˆå¹¶æ–‡ä»¶å¤±è´¥: {e}")
    
    print(f"\nğŸ“‹ æç¤ºè¯æå–æ€»ç»“:")
    print(f"   åŸºç¡€æ¨¡å‹: {'âœ… æˆåŠŸ' if baseline_success else 'âŒ å¤±è´¥'}")
    print(f"   MLflowä¼˜åŒ–æ¨¡å‹: {'âœ… æˆåŠŸ' if mlflow_success else 'âŒ ä¸å¯ç”¨'}")
    print(f"   æç¤ºè¯æ–‡ä»¶ä¼˜åŒ–æ¨¡å‹: {'âœ… æˆåŠŸ' if prompts_success else 'âŒ ä¸å¯ç”¨'}")
    print()
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("   - baseline_prompts.json (åŸºç¡€æ¨¡å‹æç¤ºè¯)")
    print("   - optimized_mlflow_prompts.json (MLflowä¼˜åŒ–æ¨¡å‹æç¤ºè¯)")  
    print("   - optimized_prompts_file_prompts.json (æç¤ºè¯æ–‡ä»¶ä¼˜åŒ–æ¨¡å‹)")
    print("   - all_prompts_for_typescript.json (TypeScriptä½¿ç”¨çš„åˆå¹¶æ–‡ä»¶)")
    print("   - prompts-types.ts (TypeScriptæ¥å£å®šä¹‰)")

def main():
    """Main function"""
    print("ğŸ‰ ä¼˜åŒ–æ¨¡å—ä½¿ç”¨æŒ‡å—")
    print("=" * 60)
    print("æœ¬è„šæœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ DSPy ä¼˜åŒ–åçš„æ¨¡å—")
    print("æ”¯æŒä» MLflow åŠ è½½å’Œä»æç¤ºè¯æ–‡ä»¶é‡å»ºä¸¤ç§æ–¹å¼")
    print("âœ¨ æ–°å¢: æ™ºèƒ½éœ€æ±‚å˜åŒ–å™¨ï¼Œé¿å…ç¼“å­˜å½±å“æµ‹è¯•ç»“æœ")
    print("âœ¨ æ–°å¢: æç¤ºè¯æå–åŠŸèƒ½ï¼Œç”¨äºTypeScripté›†æˆ")
    print()
    
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--extract-prompts":
        # Run only prompt extraction
        extract_all_prompts()
        return
    
    # Run demonstrations
    demonstrate_usage()
    production_usage_example()
    test_cache_avoidance()
    
    # Also extract prompts by default
    print("\n" + "="*60)
    extract_all_prompts()
    
    print(f"\nğŸ“‹ ä½¿ç”¨æ€»ç»“:")
    print("1. ä¼˜å…ˆä½¿ç”¨ MLFlow åŠ è½½æ–¹å¼ï¼ˆå®Œæ•´ä¿å­˜äº†ä¼˜åŒ–çŠ¶æ€ï¼‰")
    print("2. æç¤ºè¯æ–‡ä»¶é‡å»ºæ˜¯å¤‡ç”¨æ–¹æ¡ˆï¼ˆä¸»è¦ä¿å­˜äº†ç¤ºä¾‹æ¼”ç¤ºï¼‰")
    print("3. åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å»ºè®®å®ç°è‡ªåŠ¨å›é€€æœºåˆ¶")
    print("4. ä¼˜åŒ–æ¨¡å‹åŒ…å«äº†å­¦ä¹ åˆ°çš„ç¤ºä¾‹æ¼”ç¤ºï¼Œæ€§èƒ½é€šå¸¸ä¼˜äºåŸºç¡€æ¨¡å‹")
    print("5. âœ¨ ä½¿ç”¨éœ€æ±‚å˜åŒ–å™¨ç¡®ä¿æ¯æ¬¡æµ‹è¯•éƒ½æœ‰ç‹¬ç‰¹è¾“å…¥ï¼Œé¿å…ç¼“å­˜å½±å“")
    print("6. âœ¨ ä½¿ç”¨ --extract-prompts å‚æ•°ä»…è¿è¡Œæç¤ºè¯æå–")
    print()
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    main() 