import { z } from 'zod';
import { streamText, wrapLanguageModel } from 'ai';
import { TransformRepository } from './TransformRepository';
import { JsondocRepository } from './JsondocRepository';
import { createAgentTool } from './StreamingAgentFramework';
import { buildAgentConfiguration, buildPromptForRequestType } from '../services/AgentRequestBuilder';
import { getLLMModel } from './LLMConfig';
import { createUserContextMiddleware } from '../middleware/UserContextMiddleware';
import { getParticleSystem } from '../services/ParticleSystemInitializer';

// Schema for general agent requests
export const GeneralAgentRequestSchema = z.object({
    userRequest: z.string(),
    projectId: z.string(),
    contextType: z.enum(['brainstorm', 'general']).optional(),
    contextData: z.any().optional() // Additional context data
});

export type GeneralAgentRequest = z.infer<typeof GeneralAgentRequestSchema>;

export class AgentService {
    private chatMessageRepo?: any; // Injected later to avoid circular dependency

    constructor(
        private transformRepo: TransformRepository,
        private jsondocRepo: JsondocRepository,
    ) { }

    // Method to inject chat repository after initialization
    public setChatMessageRepository(chatMessageRepo: any) {
        this.chatMessageRepo = chatMessageRepo;
    }

    // Helper function to generate computation state indicators
    private generateComputationIndicator(phase: string, content?: string): string {
        const icons = ['ğŸ”„', 'âš¡', 'ğŸ¯', 'âœ¨', 'ğŸ”', 'ğŸ’¡', 'ğŸš€', 'â­'];
        const hash = this.simpleHash(content || phase);
        const icon = icons[hash % icons.length];

        const phases = {
            'thinking': 'ç¼–å‰§å¤§è„‘å¯åŠ¨ä¸­',
            'analyzing': 'å‰§æƒ…åˆ†æå™¨è¿è½¬ä¸­',
            'processing': 'è§’è‰²ä»¬æ­£åœ¨æ’ç»ƒ',
            'generating': 'åˆ›æ„ç«èŠ±å››æº…ä¸­',
            'completing': 'æœ€åæ¶¦è‰²ï¼Œé©¬ä¸Šå®Œå·¥',
            'error': 'âŒ å‰§æœ¬å¡å£³äº†'
        };

        return `${icon} ${phases[phase as keyof typeof phases] || 'æ­£åœ¨è¿›è¡Œç›¸å…³è®¡ç®—'}...`;
    }

    // Simple hash function for consistent but varied indicators
    private simpleHash(str: string): number {
        let hash = 0;
        for (let i = 0; i < str.length; i++) {
            const char = str.charCodeAt(i);
            hash = ((hash << 5) - hash) + char;
            hash = hash & hash; // Convert to 32bit integer
        }
        return Math.abs(hash);
    }

    // Helper function to parse JSON with fallback
    private async tryParseAgentJSON(text: string): Promise<{ humanReadableMessage?: string; parsed: boolean }> {
        try {
            // Remove markdown code blocks if present
            let cleanText = text.trim();
            if (cleanText.startsWith('```json')) {
                cleanText = cleanText.replace(/^```json\s*/, '').replace(/\s*```$/, '');
            }

            // Try regular JSON.parse first
            let parsed;
            try {
                parsed = JSON.parse(cleanText);
            } catch (jsonError) {
                // Fallback to jsonrepair
                const { jsonrepair } = await import('jsonrepair');
                const repairedJson = jsonrepair(cleanText);
                parsed = JSON.parse(repairedJson);
            }

            return {
                humanReadableMessage: parsed.humanReadableMessage,
                parsed: true
            };
        } catch (error) {
            return { parsed: false };
        }
    }

    /**
     * Build minimal context that provides only workflow state overview for particle-based search
     * This is used when particle search tools are available to reduce context size
     */
    private async buildMinimalContextForParticleSearch(
        projectId: string,
        jsondocRepo: JsondocRepository
    ): Promise<string> {
        const contextLines: string[] = [];
        contextLines.push('=== é¡¹ç›®çŠ¶æ€æ¦‚è§ˆ ===');

        try {
            // Get basic project state without loading full content
            const allJsondocs = await jsondocRepo.getProjectJsondocs(projectId);

            const hasInput = allJsondocs.some((j: any) => j.schema_type === 'user_input');
            const hasBrainstorm = allJsondocs.some((j: any) => j.schema_type === 'çµæ„Ÿåˆ›æ„');
            const hasOutline = allJsondocs.some((j: any) => j.schema_type === 'å‰§æœ¬è®¾å®š');
            const hasChronicles = allJsondocs.some((j: any) => j.schema_type === 'chronicles');
            const hasEpisodePlanning = allJsondocs.some((j: any) => j.schema_type === 'episode_planning');
            const episodeSynopses = allJsondocs.filter((j: any) => j.schema_type === 'episode_synopsis');

            if (hasInput) contextLines.push('âœ“ ç”¨æˆ·è¾“å…¥å·²åˆ›å»º');
            if (hasBrainstorm) contextLines.push('âœ“ æ•…äº‹åˆ›æ„å·²ç”Ÿæˆ');
            if (hasOutline) contextLines.push('âœ“ å‰§æœ¬è®¾å®šå·²ç”Ÿæˆ');
            if (hasChronicles) contextLines.push('âœ“ æ—¶é—´é¡ºåºå¤§çº²å·²ç”Ÿæˆ');
            if (hasEpisodePlanning) contextLines.push('âœ“ å‰§é›†æ¡†æ¶å·²ç”Ÿæˆ');
            if (episodeSynopses.length > 0) {
                contextLines.push(`âœ“ å·²ç”Ÿæˆ ${episodeSynopses.length} ä¸ªåˆ†é›†å¤§çº²`);
            }

            if (contextLines.length === 1) {
                contextLines.push('é¡¹ç›®å°šæœªå¼€å§‹ï¼Œå¯ä»¥ä»åˆ›å»ºæ•…äº‹åˆ›æ„å¼€å§‹');
            }

        } catch (error) {
            console.warn('[AgentService] Failed to load project state for minimal context:', error);
            contextLines.push('æ— æ³•åŠ è½½é¡¹ç›®çŠ¶æ€ï¼Œè¯·ä½¿ç”¨æŸ¥è¯¢å·¥å…·è·å–ä¿¡æ¯');
        }

        contextLines.push('');
        contextLines.push('ä½¿ç”¨ query å·¥å…·æœç´¢å…·ä½“å†…å®¹ï¼Œä½¿ç”¨ getJsondocContent å·¥å…·æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ã€‚');

        return contextLines.join('\n');
    }

    /**
     * Enhanced prompt with query guidance for particle-based search
     */
    private buildQueryGuidedPrompt(userRequest: string, minimalContext: string): string {
        return `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ­å‰§å‰§æœ¬åˆ›ä½œå’Œç¼–è¾‘åŠ©æ‰‹ã€‚ä½ æ‹¥æœ‰æ™ºèƒ½æŸ¥è¯¢å·¥å…·æ¥æŒ‰éœ€è·å–é¡¹ç›®ä¿¡æ¯ã€‚

**ç”¨æˆ·è¯·æ±‚ï¼š** "${userRequest}"

${minimalContext}

**ä½ çš„å·¥ä½œæµç¨‹ï¼š**

1. **åˆ†æç”¨æˆ·è¯·æ±‚** - ç†è§£ç”¨æˆ·çš„çœŸå®æ„å›¾å’Œéœ€è¦ä»€ä¹ˆä¿¡æ¯
2. **æ™ºèƒ½ä¿¡æ¯æ”¶é›†** - ä½¿ç”¨ query å·¥å…·æœç´¢ç›¸å…³ä¿¡æ¯
   - ä¾‹å¦‚ï¼šquery("è§’è‰²è®¾å®š") æ¥æŸ¥æ‰¾äººç‰©ä¿¡æ¯
   - ä¾‹å¦‚ï¼šquery("æ•…äº‹èƒŒæ™¯") æ¥äº†è§£è®¾å®šä¿¡æ¯
   - ä¾‹å¦‚ï¼šquery("å‰§é›†ç»“æ„") æ¥æŸ¥çœ‹åˆ†é›†å®‰æ’
3. **æ·±å…¥å†…å®¹è·å–** - ä½¿ç”¨ getJsondocContent å·¥å…·è·å–å®Œæ•´æ–‡æ¡£
4. **å†³ç­–å’Œæ‰§è¡Œ** - åŸºäºæ”¶é›†çš„ä¿¡æ¯é€‰æ‹©åˆé€‚çš„å·¥å…·æ‰§è¡Œä»»åŠ¡

**æŸ¥è¯¢ç­–ç•¥æŒ‡å¯¼ï¼š**
- å¯¹äºç¼–è¾‘ä»»åŠ¡ï¼šå…ˆæŸ¥è¯¢ç›¸å…³çš„ç°æœ‰å†…å®¹
- å¯¹äºç”Ÿæˆä»»åŠ¡ï¼šæŸ¥è¯¢æ‰€éœ€çš„ä¸Šæ¸¸ä¾èµ–ä¿¡æ¯
- å¯¹äºé—®ç­”ä»»åŠ¡ï¼šæŸ¥è¯¢ç”¨æˆ·è¯¢é—®çš„å…·ä½“å†…å®¹
- å¯ä»¥è¿›è¡Œå¤šæ¬¡æŸ¥è¯¢æ¥å…¨é¢äº†è§£æƒ…å†µ

**é‡è¦åŸåˆ™ï¼š**
- æ ¹æ®æŸ¥è¯¢ç»“æœä¸­çš„ similarity åˆ†æ•°åˆ¤æ–­ä¿¡æ¯ç›¸å…³æ€§
- similarity > 0.7: é«˜åº¦ç›¸å…³ï¼Œå¯ç›´æ¥ä½¿ç”¨
- similarity 0.4-0.7: ä¸­ç­‰ç›¸å…³ï¼Œéœ€è¦è¿›ä¸€æ­¥ç¡®è®¤
- similarity < 0.4: ä½ç›¸å…³ï¼Œå¯èƒ½éœ€è¦é‡æ–°æŸ¥è¯¢

**å¯ç”¨å·¥å…·è¯´æ˜ï¼š**
- query: è¯­ä¹‰æœç´¢é¡¹ç›®ä¸­çš„ç›¸å…³ä¿¡æ¯
- getJsondocContent: è·å–æŒ‡å®šæ–‡æ¡£çš„å®Œæ•´å†…å®¹
- å…¶ä»–åˆ›ä½œå·¥å…·ä¼šæ ¹æ®é¡¹ç›®çŠ¶æ€è‡ªåŠ¨æä¾›

å¼€å§‹åˆ†æç”¨æˆ·è¯·æ±‚ï¼Œè¿›è¡Œå¿…è¦çš„ä¿¡æ¯æŸ¥è¯¢ï¼Œç„¶åæ‰§è¡Œç›¸åº”çš„å·¥å…·ã€‚`;
    }

    /**
     * Generate user-friendly error messages based on error type and content
     */
    private generateUserFriendlyErrorMessage(error: any): string {
        const errorMessage = error instanceof Error ? error.message : String(error);
        const errorString = String(error);

        // Content filtering errors
        if (errorMessage.includes('inappropriate content') ||
            errorMessage.includes('data_inspection_failed') ||
            errorString.includes('data_inspection_failed')) {
            return 'æŠ±æ­‰ï¼Œæ‚¨çš„å†…å®¹å¯èƒ½åŒ…å«äº†ä¸é€‚å®œçš„ä¿¡æ¯ï¼Œæ— æ³•å¤„ç†ã€‚è¯·å°è¯•ä¿®æ”¹æ‚¨çš„åˆ›æ„å†…å®¹ï¼Œé¿å…ä½¿ç”¨å¯èƒ½è¢«è¯¯åˆ¤çš„è¯æ±‡ï¼ˆå¦‚"è¡—å¤´éœ¸ç‹"ç­‰å¯èƒ½ä¸æš´åŠ›ç›¸å…³çš„è¯æ±‡ï¼‰ï¼Œç„¶åé‡æ–°æäº¤ã€‚';
        }

        // API rate limiting
        if (errorMessage.includes('rate limit') || errorMessage.includes('too many requests')) {
            return 'è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•ã€‚æˆ‘ä»¬æ­£åœ¨åŠªåŠ›ä¸ºæ‚¨å¤„ç†è¯·æ±‚ã€‚';
        }

        // Network/connection errors
        if (errorMessage.includes('network') || errorMessage.includes('connection') ||
            errorMessage.includes('timeout') || errorMessage.includes('ECONNRESET')) {
            return 'ç½‘ç»œè¿æ¥å‡ºç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥åé‡è¯•ã€‚å¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·ç¨åå†è¯•ã€‚';
        }

        // Authentication errors
        if (errorMessage.includes('authentication') || errorMessage.includes('unauthorized') ||
            errorMessage.includes('invalid token')) {
            return 'èº«ä»½éªŒè¯å¤±è´¥ï¼Œè¯·åˆ·æ–°é¡µé¢åé‡è¯•ã€‚å¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·è”ç³»æ”¯æŒã€‚';
        }

        // Model/API service errors
        if (errorMessage.includes('model') || errorMessage.includes('service unavailable') ||
            errorMessage.includes('internal server error')) {
            return 'AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚æˆ‘ä»¬æ­£åœ¨åŠªåŠ›æ¢å¤æœåŠ¡ã€‚';
        }

        // Quota/billing errors
        if (errorMessage.includes('quota') || errorMessage.includes('billing') ||
            errorMessage.includes('insufficient funds')) {
            return 'æœåŠ¡é…é¢å·²ç”¨å®Œï¼Œè¯·è”ç³»ç®¡ç†å‘˜æˆ–ç¨åé‡è¯•ã€‚';
        }

        // Streaming/data processing errors
        if (errorMessage.includes('No data received from streaming') ||
            errorMessage.includes('stream') || errorMessage.includes('ReadableStream')) {
            return 'æ•°æ®æµå¤„ç†å‡ºç°é—®é¢˜ï¼Œè¯·é‡è¯•ã€‚å¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œå¯èƒ½æ˜¯æœåŠ¡å™¨æ­£åœ¨ç»´æŠ¤ã€‚';
        }

        // Schema validation errors
        if (errorMessage.includes('schema') || errorMessage.includes('validation') ||
            errorMessage.includes('parse')) {
            return 'æ•°æ®æ ¼å¼éªŒè¯å¤±è´¥ï¼Œè¯·é‡è¯•ã€‚å¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚';
        }

        // Default fallback message
        return 'æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·é‡è¯•ï¼Œå¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·è”ç³»æ”¯æŒã€‚';
    }

    /**
     * Check if request is a continuation and reconstruct conversation history
     */
    private async checkForContinuation(
        request: GeneralAgentRequest,
        projectId: string
    ): Promise<{ isContinuation: boolean; conversationHistory: Array<{ role: string; content: string }> }> {
        // Check if this is a continuation request (contains keywords like "continue", "next", or specific episode ranges)
        const userRequest = request.userRequest.toLowerCase();
        const isContinuation = userRequest.includes('continue') ||
            userRequest.includes('next') ||
            userRequest.includes('æ¥ä¸‹æ¥') ||
            userRequest.includes('ç»§ç»­') ||
            /ç¬¬\s*\d+\s*-\s*\d+\s*é›†/.test(userRequest); // Pattern like "ç¬¬7-12é›†"

        if (!isContinuation || !this.chatMessageRepo) {
            return { isContinuation: false, conversationHistory: [] };
        }

        // For episode synopsis generation, try to reconstruct history
        const toolName = 'generate_episode_synopsis';
        const hasExisting = await this.chatMessageRepo.hasExistingConversation(projectId, toolName);

        if (!hasExisting) {
            return { isContinuation: false, conversationHistory: [] };
        }

        // Reconstruct conversation history with continuation parameters
        const continuationParams = {
            userRequest: request.userRequest,
            contextType: request.contextType,
            contextData: request.contextData
        };

        const conversationHistory = await this.chatMessageRepo.reconstructHistoryForAction(
            projectId,
            toolName,
            continuationParams
        );

        return { isContinuation: true, conversationHistory };
    }

    /**
     * Build multi-message prompt from conversation history
     */
    private buildMultiMessagePrompt(conversationHistory: Array<{ role: string; content: string }>): string {
        // Convert conversation history to a single prompt that preserves the conversation structure
        // This maintains compatibility with the existing streamText interface while enabling caching

        console.log(`[AgentService] Building continuation prompt from ${conversationHistory.length} previous messages`);

        const conversationText = conversationHistory.map((msg, index) => {
            const roleLabel = msg.role === 'user' ? 'ç”¨æˆ·' :
                msg.role === 'assistant' ? 'åŠ©æ‰‹' :
                    msg.role === 'system' ? 'ç³»ç»Ÿ' : 'æœªçŸ¥';

            // For system messages, add a marker to indicate it was the previous system prompt
            const prefix = msg.role === 'system' ? '[ä¹‹å‰çš„ç³»ç»Ÿæç¤º]' : `[${roleLabel}]`;

            // Truncate very long messages for logging
            const contentPreview = msg.content.length > 200 ?
                msg.content.substring(0, 200) + '...' : msg.content;
            console.log(`[AgentService] Conversation history ${index + 1}: ${prefix} (${msg.content.length} chars): ${contentPreview}`);

            return `${prefix}: ${msg.content}`;
        }).join('\n\n');

        const continuationPrompt = `ä»¥ä¸‹æ˜¯ä¹‹å‰çš„å¯¹è¯å†å²ï¼Œè¯·åŸºäºæ­¤ä¸Šä¸‹æ–‡ç»§ç»­å¤„ç†ç”¨æˆ·çš„æ–°è¯·æ±‚ï¼š

${conversationText}

è¯·æ ¹æ®ä¸Šè¿°å¯¹è¯å†å²å’Œæœ€æ–°çš„ç”¨æˆ·è¯·æ±‚ï¼Œç»§ç»­æ‰§è¡Œç›¸åº”çš„å·¥å…·è°ƒç”¨ã€‚`;

        console.log(`[AgentService] Final continuation prompt length: ${continuationPrompt.length} characters`);

        return continuationPrompt;
    }

    /**
     * Save conversation history after successful tool execution
     */
    private async saveConversationHistory(
        projectId: string,
        toolName: string,
        toolCallId: string,
        systemPromptSentToLLM: string,
        toolResult: any,
        assistantResponse: string
    ): Promise<void> {
        if (!this.chatMessageRepo) return;

        // Save the complete conversation that was sent to/from the LLM
        // This represents the actual conversation format: system prompt -> assistant response
        const messages = [
            { role: 'system', content: systemPromptSentToLLM }, // The complete system prompt sent to LLM
            { role: 'assistant', content: assistantResponse }   // The assistant's response
        ];

        await this.chatMessageRepo.saveConversation(projectId, toolName, toolCallId, messages);
    }

    /**
     * General agent method that can handle various types of requests including brainstorm editing
     */
    public async runGeneralAgent(
        projectId: string,
        userId: string,
        request: GeneralAgentRequest,
        options: {
            createChatMessages?: boolean;
            existingThinkingMessageId?: string;
            existingThinkingStartTime?: string;
            // Caching options for reproducible testing
            enableCaching?: boolean;
            seed?: number;
            temperature?: number;
            topP?: number;
            maxTokens?: number;
        } = { createChatMessages: true }
    ) {
        let computationMessageId: string | undefined;
        let responseMessageId: string | undefined;
        let accumulatedResponse = '';

        // Generate unique execution ID to correlate all messages in this agent run
        const executionId = `exec_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
        console.log(`[AgentService] Starting agent execution with ID: ${executionId}`);

        try {
            // Handle chat messages based on options
            if (options.createChatMessages && this.chatMessageRepo) {
                // Create user message event (only if not called from ChatService)
                await this.chatMessageRepo.createUserMessage(projectId, request.userRequest);

                // Create computation message for internal processing
                const computationMessage = await this.chatMessageRepo.createComputationMessage(
                    projectId,
                    this.generateComputationIndicator('thinking')
                );
                computationMessageId = computationMessage.id;

                // Note: Response message will be created when we have actual content
                responseMessageId = undefined;
            }

            // 1. Check for conversation continuation
            const { isContinuation, conversationHistory } = await this.checkForContinuation(request, projectId);

            // 2. Check if particle search is available for optimized context approach
            let useParticleSearchApproach = false;
            try {
                const particleSystem = getParticleSystem();
                if (particleSystem && particleSystem.unifiedSearch) {
                    // Check if particle system is healthy
                    const healthCheck = await particleSystem.unifiedSearch.healthCheck();
                    useParticleSearchApproach = healthCheck.particleCount > 0;
                    console.log(`[AgentService] Particle search approach ${useParticleSearchApproach ? 'enabled' : 'disabled'} (particles: ${healthCheck.particleCount})`);
                }
            } catch (error) {
                console.warn('[AgentService] Particle system health check failed:', error);
            }

            let agentConfig;
            let completePrompt;

            if (useParticleSearchApproach && !isContinuation) {
                // Use minimal context + particle search approach
                console.log('[AgentService] Using particle-search optimized approach');

                // Build minimal context
                const minimalContext = await this.buildMinimalContextForParticleSearch(projectId, this.jsondocRepo);

                // Build query-guided prompt
                completePrompt = this.buildQueryGuidedPrompt(request.userRequest, minimalContext);

                // Build agent configuration with minimal context
                agentConfig = await buildAgentConfiguration(
                    request,
                    projectId,
                    this.transformRepo,
                    this.jsondocRepo,
                    userId,
                    {
                        enableCaching: options.enableCaching,
                        seed: options.seed,
                        temperature: options.temperature,
                        topP: options.topP,
                        maxTokens: options.maxTokens
                    }
                );

                console.log(`[AgentService] Minimal context size: ${minimalContext.length} characters (vs traditional approach)`);
            } else {
                // Use traditional full context approach
                console.log('[AgentService] Using traditional full-context approach');

                // Build agent configuration using existing abstraction
                agentConfig = await buildAgentConfiguration(
                    request,
                    projectId,
                    this.transformRepo,
                    this.jsondocRepo,
                    userId,
                    {
                        enableCaching: options.enableCaching,
                        seed: options.seed,
                        temperature: options.temperature,
                        topP: options.topP,
                        maxTokens: options.maxTokens
                    }
                );

                // Use conversation history for continuation or regular prompt
                completePrompt = isContinuation && conversationHistory.length > 0
                    ? this.buildMultiMessagePrompt(conversationHistory)
                    : agentConfig.prompt;
            }

            const toolDefinitions = agentConfig.tools;

            console.log(`[AgentService] ${isContinuation ? 'Continuation' : 'New'} request detected`);
            if (isContinuation) {
                console.log(`[AgentService] Using conversation history with ${conversationHistory.length} messages`);
            }

            // Update computation message to show analysis phase
            if (computationMessageId && this.chatMessageRepo) {
                await this.chatMessageRepo.updateComputationMessage(
                    computationMessageId,
                    this.generateComputationIndicator('analyzing', request.userRequest)
                );
            }

            // 4. Save user request as raw message
            if (this.chatMessageRepo) {
                await this.chatMessageRepo.createRawMessage(
                    projectId,
                    'user',
                    request.userRequest,
                    { metadata: { source: 'streaming_agent', executionId } }
                );

                // Save the complete system prompt sent to the LLM
                await this.chatMessageRepo.createRawMessage(
                    projectId,
                    'system',
                    completePrompt,
                    {
                        metadata: {
                            source: 'streaming_agent',
                            executionId,
                            promptType: isContinuation ? 'continuation_prompt' : 'initial_prompt',
                            useParticleSearch: useParticleSearchApproach,
                            conversationHistoryLength: conversationHistory.length
                        }
                    }
                );
            }

            // 2. Run the streaming agent directly
            console.log('--- Starting Streaming Agent ---');

            // Create agent tools with context information
            const tools: Record<string, any> = {};
            const contextInfo = { projectId, userId };
            for (const toolDef of toolDefinitions) {
                tools[toolDef.name] = createAgentTool(toolDef, contextInfo);
            }

            // Extract caching options
            const {
                enableCaching = false,
                seed,
                temperature,
                topP,
                maxTokens
            } = options;

            const baseModel = await getLLMModel();

            // Create user context middleware to inject original user request into all LLM calls
            const userContextMiddleware = createUserContextMiddleware({
                originalUserRequest: request.userRequest,
                projectId,
                userId,
                timestamp: new Date().toISOString()
            });

            // Wrap model with middleware to ensure tools have access to original user context
            const enhancedModel = wrapLanguageModel({
                model: baseModel,
                middleware: userContextMiddleware
            });

            console.log('[AgentService] Using enhanced model with user context middleware');
            console.log('[AgentService] Original user request length:', request.userRequest.length);

            const result = await streamText({
                model: enhancedModel, // Use enhanced model instead of base model
                tools: tools,
                maxSteps: 25, // Allow more steps for complex editing workflows
                maxTokens: 32768,
                prompt: completePrompt,
                // Pass AI SDK options directly
                ...(seed && { seed }),
                ...(temperature && { temperature }),
                ...(topP && { topP }),
                // ...(maxTokens && { maxTokens })
            });

            console.log('\n\n--- Agent Stream & Final Output ---');
            let finalResponse = '';
            let currentToolCall: any = null;
            let toolCallCount = 0;
            let toolResultCount = 0;

            for await (const delta of result.fullStream) {
                switch (delta.type) {
                    case 'text-delta':
                        process.stdout.write(delta.textDelta);
                        accumulatedResponse += delta.textDelta;
                        finalResponse += delta.textDelta;

                        // Create response message on first content if not already created
                        if (!responseMessageId && options.createChatMessages && this.chatMessageRepo && accumulatedResponse.trim()) {
                            const responseMessage = await this.chatMessageRepo.createResponseMessage(projectId, accumulatedResponse);
                            responseMessageId = responseMessage.id;
                        }

                        // Try to parse JSON and update response message in real-time
                        if (responseMessageId && this.chatMessageRepo) {
                            const parseResult = await this.tryParseAgentJSON(accumulatedResponse);
                            if (parseResult.parsed && parseResult.humanReadableMessage) {
                                // Successfully parsed JSON, update with human readable message
                                await this.chatMessageRepo.updateResponseMessage(
                                    responseMessageId,
                                    parseResult.humanReadableMessage,
                                    'streaming'
                                );
                            } else {
                                // JSON not complete yet, show accumulated text
                                await this.chatMessageRepo.updateResponseMessage(
                                    responseMessageId,
                                    accumulatedResponse,
                                    'streaming'
                                );
                            }
                        }
                        break;

                    case 'tool-call':
                        console.log(`\n[Agent Action] Starting tool call to '${delta.toolName}' with ID '${delta.toolCallId}'`);
                        currentToolCall = delta;
                        toolCallCount++;

                        // Update computation message to show processing phase
                        if (computationMessageId && this.chatMessageRepo) {
                            await this.chatMessageRepo.updateComputationMessage(
                                computationMessageId,
                                this.generateComputationIndicator('processing', delta.toolName)
                            );
                        }

                        // Save tool call as raw message
                        if (this.chatMessageRepo && projectId) {
                            await this.chatMessageRepo.createRawMessage(
                                projectId,
                                'tool',
                                `Tool call: ${delta.toolName}`,
                                {
                                    toolName: delta.toolName,
                                    toolParameters: delta.args,
                                    metadata: {
                                        toolCallId: delta.toolCallId,
                                        source: 'streaming_agent',
                                        executionId
                                    }
                                }
                            );
                        }
                        break;

                    case 'tool-result':
                        console.log(`\n[Agent Action] Received result for tool call '${delta.toolCallId}'`);
                        toolResultCount++;

                        // Update computation message to show generation phase
                        if (computationMessageId && this.chatMessageRepo) {
                            await this.chatMessageRepo.updateComputationMessage(
                                computationMessageId,
                                this.generateComputationIndicator('generating', currentToolCall?.toolName)
                            );
                        }

                        // Save tool result as raw message
                        if (this.chatMessageRepo && projectId && currentToolCall) {
                            await this.chatMessageRepo.createRawMessage(
                                projectId,
                                'tool',
                                `Tool result: ${currentToolCall.toolName}`,
                                {
                                    toolName: currentToolCall.toolName,
                                    toolParameters: currentToolCall.args,
                                    toolResult: delta.result,
                                    metadata: {
                                        toolCallId: delta.toolCallId,
                                        source: 'streaming_agent',
                                        executionId
                                    }
                                }
                            );

                            // Save conversation history for episode synopsis tool
                            if (currentToolCall.toolName === 'generate_episode_synopsis') {
                                await this.saveConversationHistory(
                                    projectId,
                                    currentToolCall.toolName,
                                    delta.toolCallId,
                                    completePrompt, // The complete system prompt that was sent to the LLM
                                    delta.result,
                                    accumulatedResponse || 'Tool executed successfully'
                                );
                            }
                        }
                        break;

                    case 'step-finish':
                        console.log(`\n[Agent Step] Step completed with reason: ${delta.finishReason}`);
                        // These are completion events for agent reasoning/tool-calling steps
                        // They're informational and don't require specific handling
                        break;

                    case 'step-start':
                        console.log(`\n[Agent Step] Step started`);
                        // These are start events for agent reasoning/tool-calling steps
                        // They're informational and don't require specific handling
                        break;

                    case 'error':
                        console.log(`\n[Agent Error] Error occurred:`, (delta as any).error?.message || 'Unknown error');
                        // Error events during agent processing
                        // The error will be handled by the outer try-catch
                        break;

                    case 'finish':
                        console.log(`\n[Agent Finish] Agent completed with reason: ${delta.finishReason}`);
                        // Final completion event for the entire agent execution
                        // This is informational as the result is handled elsewhere
                        break;

                    default:
                        console.log(`[DEBUG] Unhandled delta type: ${delta.type}`, delta);
                        break;
                }
            }
            console.log('\n-----------------------------------');

            // Final processing of the response
            if (options.createChatMessages && this.chatMessageRepo) {
                // Create response message if it wasn't created during streaming (e.g., tool-only responses)
                if (!responseMessageId) {
                    const defaultMessage = toolCallCount > 0 ? 'æˆ‘å·²å®Œæˆæ‚¨çš„è¯·æ±‚ã€‚' : 'æ”¶åˆ°æ‚¨çš„è¯·æ±‚ï¼Œæ­£åœ¨å¤„ç†ä¸­...';
                    const responseMessage = await this.chatMessageRepo.createResponseMessage(projectId, defaultMessage);
                    responseMessageId = responseMessage.id;
                }

                if (responseMessageId) {
                    // Try both accumulatedResponse and finalResponse
                    let finalParseResult = await this.tryParseAgentJSON(accumulatedResponse);
                    if (!finalParseResult.parsed && finalResponse !== accumulatedResponse) {
                        finalParseResult = await this.tryParseAgentJSON(finalResponse);
                    }

                    if (finalParseResult.parsed && finalParseResult.humanReadableMessage) {
                        // Successfully parsed JSON, use human readable message
                        await this.chatMessageRepo.updateResponseMessage(
                            responseMessageId,
                            finalParseResult.humanReadableMessage,
                            'completed'
                        );

                        // Add a small delay and force another update to ensure it's persisted
                        await new Promise(resolve => setTimeout(resolve, 100));
                        await this.chatMessageRepo.updateResponseMessage(
                            responseMessageId,
                            finalParseResult.humanReadableMessage,
                            'completed'
                        );
                    } else {
                        // JSON parsing failed, use accumulated text as fallback
                        const fallbackMessage = accumulatedResponse || finalResponse || (toolCallCount > 0 ? 'æˆ‘å·²å®Œæˆæ‚¨çš„è¯·æ±‚ã€‚' : 'æ”¶åˆ°æ‚¨çš„è¯·æ±‚ã€‚');
                        await this.chatMessageRepo.updateResponseMessage(
                            responseMessageId,
                            fallbackMessage,
                            'completed'
                        );
                    }
                }
            }

            // Complete computation message
            if (computationMessageId && this.chatMessageRepo) {
                await this.chatMessageRepo.updateComputationMessage(
                    computationMessageId,
                    this.generateComputationIndicator('completing'),
                    'completed'
                );
            }

            // Save final assistant response as raw message
            if (this.chatMessageRepo && projectId && finalResponse.trim()) {
                await this.chatMessageRepo.createRawMessage(
                    projectId,
                    'assistant',
                    finalResponse.trim(),
                    { metadata: { source: 'streaming_agent', executionId } }
                );
            }

            const finishReason = await result.finishReason;

            // Log summary using our tracked counts
            console.log(`\n--- Agent Execution Summary ---`);
            console.log(`Finish Reason: ${finishReason}`);
            console.log(`\nTool Calls Made: ${toolCallCount}`);
            console.log(`\nTool Results Received by Agent: ${toolResultCount}`);

            console.log(`[AgentService] General agent completed for project ${projectId}.`);

        } catch (error) {
            console.error("\n--- Agent Flow Failed ---");
            console.error(`[AgentService] General agent failed for project ${projectId}:`, error);

            // Generate user-friendly error message based on error type
            const userFriendlyMessage = this.generateUserFriendlyErrorMessage(error);

            // Update messages with error state
            if (computationMessageId && this.chatMessageRepo) {
                await this.chatMessageRepo.updateComputationMessage(
                    computationMessageId,
                    this.generateComputationIndicator('error'),
                    'failed'
                );
            }

            if (options.createChatMessages && this.chatMessageRepo) {
                // Create response message if it wasn't created yet
                if (!responseMessageId) {
                    const responseMessage = await this.chatMessageRepo.createResponseMessage(projectId, userFriendlyMessage);
                    responseMessageId = responseMessage.id;
                } else {
                    await this.chatMessageRepo.updateResponseMessage(
                        responseMessageId,
                        userFriendlyMessage,
                        'failed'
                    );
                }
            }

            // Save error as raw message
            if (this.chatMessageRepo && projectId) {
                await this.chatMessageRepo.createRawMessage(
                    projectId,
                    'assistant',
                    `Error: ${error instanceof Error ? error.message : String(error)}`,
                    { metadata: { source: 'streaming_agent', error: true, executionId } }
                );
            }

            throw error;
        }
    }

    /**
     * Health check for particle-based search capabilities
     */
    public async checkParticleSearchHealth(): Promise<{
        particleSystemAvailable: boolean;
        unifiedSearchAvailable: boolean;
        searchModes: {
            stringSearchAvailable: boolean;
            embeddingSearchAvailable: boolean;
        };
        particleCount: number;
    }> {
        try {
            const particleSystem = getParticleSystem();
            if (!particleSystem) {
                return {
                    particleSystemAvailable: false,
                    unifiedSearchAvailable: false,
                    searchModes: {
                        stringSearchAvailable: false,
                        embeddingSearchAvailable: false
                    },
                    particleCount: 0
                };
            }

            const healthCheck = await particleSystem.unifiedSearch.healthCheck();

            return {
                particleSystemAvailable: true,
                unifiedSearchAvailable: true,
                searchModes: {
                    stringSearchAvailable: healthCheck.stringSearchAvailable,
                    embeddingSearchAvailable: healthCheck.embeddingSearchAvailable
                },
                particleCount: healthCheck.particleCount
            };
        } catch (error) {
            console.error('[AgentService] Particle search health check failed:', error);
            return {
                particleSystemAvailable: false,
                unifiedSearchAvailable: false,
                searchModes: {
                    stringSearchAvailable: false,
                    embeddingSearchAvailable: false
                },
                particleCount: 0
            };
        }
    }
} 