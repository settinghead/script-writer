import dspy
from typing import List, Dict, Tuple, Optional
from common import StoryIdea, BrainstormRequest, EvaluationResult, format_ideas_for_evaluation, eval_lm
import hashlib
import json
import pickle
import os

class NoveltyEvaluationSignature(dspy.Signature):
    """Evaluate novelty and originality of story ideas"""
    genre = dspy.InputField(desc="æ•…äº‹é¢˜æç±»åž‹")
    story_ideas = dspy.InputField(desc="å¾…è¯„ä¼°çš„æ•…äº‹åˆ›æ„åˆ—è¡¨")
    novelty_score = dspy.OutputField(desc="æ–°é¢–æ€§è¯„åˆ†(1-10åˆ†)ï¼Œè¯„ä¼°åˆ›æ„çš„åŽŸåˆ›æ€§å’Œé¿å…å¥—è·¯ç¨‹åº¦")
    novelty_feedback = dspy.OutputField(desc="æ–°é¢–æ€§è¯„ä»·åé¦ˆï¼ŒæŒ‡å‡ºå“ªäº›åˆ›æ„æ–°é¢–ï¼Œå“ªäº›å¥—è·¯åŒ–")

class FeasibilityEvaluationSignature(dspy.Signature):
    """Evaluate production feasibility of story ideas"""
    platform = dspy.InputField(desc="ç›®æ ‡å¹³å°")
    story_ideas = dspy.InputField(desc="å¾…è¯„ä¼°çš„æ•…äº‹åˆ›æ„åˆ—è¡¨")
    feasibility_score = dspy.OutputField(desc="æ‹æ‘„å¯è¡Œæ€§è¯„åˆ†(1-10åˆ†)ï¼Œè€ƒè™‘æˆæœ¬ã€åœºæ™¯ã€æ¼”å‘˜ç­‰å› ç´ ")
    feasibility_feedback = dspy.OutputField(desc="å¯è¡Œæ€§è¯„ä»·åé¦ˆï¼Œåˆ†æžåˆ¶ä½œéš¾åº¦å’Œå®žé™…çº¦æŸ")

class StructureEvaluationSignature(dspy.Signature):
    """Evaluate structural clarity of story ideas"""
    story_ideas = dspy.InputField(desc="å¾…è¯„ä¼°çš„æ•…äº‹åˆ›æ„åˆ—è¡¨")
    structure_score = dspy.OutputField(desc="ç»“æž„æ˜Žæ™°åº¦è¯„åˆ†(1-10åˆ†)ï¼Œè¯„ä¼°èµ·æ‰¿è½¬åˆçš„å®Œæ•´æ€§")
    structure_feedback = dspy.OutputField(desc="ç»“æž„è¯„ä»·åé¦ˆï¼Œåˆ†æžæ•…äº‹ç»“æž„çš„æ¸…æ™°åº¦å’Œé€»è¾‘æ€§")

class DetailEvaluationSignature(dspy.Signature):
    """Evaluate the level of detail in story ideas"""
    story_ideas = dspy.InputField(desc="å¾…è¯„ä¼°çš„æ•…äº‹åˆ›æ„åˆ—è¡¨")
    detail_score = dspy.OutputField(desc="è¯¦ç»†ç¨‹åº¦è¯„åˆ†(1-10åˆ†)ï¼Œè¯„ä¼°æ•…äº‹æ¢—æ¦‚çš„ä¸°å¯Œæ€§ã€ç»†èŠ‚æè¿°å’Œæƒ…èŠ‚å±•å¼€ç¨‹åº¦")
    detail_feedback = dspy.OutputField(desc="è¯¦ç»†ç¨‹åº¦è¯„ä»·åé¦ˆï¼Œåˆ†æžå“ªäº›åˆ›æ„æè¿°å……åˆ†ï¼Œå“ªäº›è¿‡äºŽç®€ç•¥")

class LogicalCoherenceEvaluationSignature(dspy.Signature):
    """Evaluate logical coherence and internal consistency of story ideas"""
    genre = dspy.InputField(desc="æ•…äº‹é¢˜æç±»åž‹")
    story_ideas = dspy.InputField(desc="å¾…è¯„ä¼°çš„æ•…äº‹åˆ›æ„åˆ—è¡¨")
    logical_coherence_score = dspy.OutputField(desc="é€»è¾‘è¿žè´¯æ€§è¯„åˆ†(1-10åˆ†)ï¼Œè¯„ä¼°æ•…äº‹å†…åœ¨é€»è¾‘ã€æ—¶é—´çº¿ä¸€è‡´æ€§ã€å› æžœå…³ç³»åˆç†æ€§ï¼Œç‰¹åˆ«å…³æ³¨ç©¿è¶Šã€é‡ç”Ÿã€å¤šæ—¶ç©ºç­‰å¤æ‚è®¾å®šçš„é€»è¾‘æ¼æ´ž")
    logical_coherence_feedback = dspy.OutputField(desc="é€»è¾‘è¿žè´¯æ€§è¯„ä»·åé¦ˆï¼ŒæŒ‡å‡ºæ•…äº‹ä¸­çš„é€»è¾‘æ¼æ´žã€æ—¶é—´çº¿çŸ›ç›¾ã€å› æžœå…³ç³»ä¸åˆç†ç­‰é—®é¢˜")

class GenreConsistencySignature(dspy.Signature):
    """Evaluate genre consistency of story ideas"""
    genre = dspy.InputField(desc="é¢„æœŸçš„æ•…äº‹é¢˜æç±»åž‹")
    story_ideas = dspy.InputField(desc="å¾…è¯„ä¼°çš„æ•…äº‹åˆ›æ„åˆ—è¡¨")
    genre_score = dspy.OutputField(desc="é¢˜æä¸€è‡´æ€§è¯„åˆ†(1-10åˆ†)ï¼Œè¯„ä¼°ä¸ŽæŒ‡å®šé¢˜æçš„åŒ¹é…åº¦")
    genre_feedback = dspy.OutputField(desc="é¢˜æä¸€è‡´æ€§åé¦ˆï¼Œåˆ†æžæ˜¯å¦ç¬¦åˆé¢˜æç‰¹å¾")

class EngagementEvaluationSignature(dspy.Signature):
    """Evaluate engagement potential of story ideas"""
    platform = dspy.InputField(desc="ç›®æ ‡å¹³å°")
    story_ideas = dspy.InputField(desc="å¾…è¯„ä¼°çš„æ•…äº‹åˆ›æ„åˆ—è¡¨")
    engagement_score = dspy.OutputField(desc="å¸å¼•åŠ›è¯„åˆ†(1-10åˆ†)ï¼Œè¯„ä¼°è§‚ä¼—å…´è¶£å’Œæƒ…æ„Ÿå…±é¸£")
    engagement_feedback = dspy.OutputField(desc="å¸å¼•åŠ›è¯„ä»·åé¦ˆï¼Œåˆ†æžè§‚ä¼—æŽ¥å—åº¦å’Œä¼ æ’­æ½œåŠ›")

class StoryIdeaEvaluator:
    """Comprehensive evaluator for story ideas using multiple LLM judges"""
    
    def __init__(self, cache_file: str = "evaluation_cache.pkl"):
        # Configure evaluators to use evaluation LLM
        with dspy.context(lm=eval_lm):
            self.novelty_evaluator = dspy.Predict(NoveltyEvaluationSignature)
            self.feasibility_evaluator = dspy.Predict(FeasibilityEvaluationSignature)
            self.structure_evaluator = dspy.Predict(StructureEvaluationSignature)
            self.detail_evaluator = dspy.Predict(DetailEvaluationSignature)
            self.logical_coherence_evaluator = dspy.Predict(LogicalCoherenceEvaluationSignature)
            self.genre_evaluator = dspy.Predict(GenreConsistencySignature)
            self.engagement_evaluator = dspy.Predict(EngagementEvaluationSignature)
        
        # Initialize cache
        self.cache_file = cache_file
        self.cache = self._load_cache()
    
    def _load_cache(self) -> Dict[str, EvaluationResult]:
        """Load evaluation cache from disk"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                print(f"Warning: Failed to load cache: {e}")
                return {}
        return {}
    
    def _save_cache(self):
        """Save evaluation cache to disk"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache, f)
        except Exception as e:
            print(f"Warning: Failed to save cache: {e}")
    
    def _get_cache_key(self, ideas: List[StoryIdea], request: BrainstormRequest) -> str:
        """Generate a cache key for the given ideas and request"""
        content = {
            'ideas': [{'title': idea.title, 'body': idea.body} for idea in ideas],
            'genre': request.genre,
            'platform': request.platform,
            'requirements': request.requirements_section
        }
        content_str = json.dumps(content, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(content_str.encode('utf-8')).hexdigest()
    
    def evaluate(self, ideas: List[StoryIdea], request: BrainstormRequest) -> EvaluationResult:
        """Comprehensive evaluation of story ideas with caching"""
        
        # Check cache first
        cache_key = self._get_cache_key(ideas, request)
        if cache_key in self.cache:
            print("  ðŸ“‹ ä½¿ç”¨ç¼“å­˜çš„è¯„ä¼°ç»“æžœ")
            return self.cache[cache_key]
        
        formatted_ideas = format_ideas_for_evaluation(ideas)
        
        # Use evaluation LLM for all evaluations
        with dspy.context(lm=eval_lm):
            # Evaluate novelty
            novelty_result = self.novelty_evaluator(
                genre=request.genre,
                story_ideas=formatted_ideas
            )
            
            # Evaluate feasibility
            feasibility_result = self.feasibility_evaluator(
                platform=request.platform,
                story_ideas=formatted_ideas
            )
            
            # Evaluate structure
            structure_result = self.structure_evaluator(
                story_ideas=formatted_ideas
            )
            
            # Evaluate detail level
            detail_result = self.detail_evaluator(
                story_ideas=formatted_ideas
            )
            
            # Evaluate logical coherence
            logical_coherence_result = self.logical_coherence_evaluator(
                genre=request.genre,
                story_ideas=formatted_ideas
            )
            
            # Evaluate genre consistency
            genre_result = self.genre_evaluator(
                genre=request.genre,
                story_ideas=formatted_ideas
            )
            
            # Evaluate engagement
            engagement_result = self.engagement_evaluator(
                platform=request.platform,
                story_ideas=formatted_ideas
            )
        
        # Parse scores (handle potential parsing errors)
        novelty_score = self._parse_score(novelty_result.novelty_score)
        feasibility_score = self._parse_score(feasibility_result.feasibility_score)
        structure_score = self._parse_score(structure_result.structure_score)
        detail_score = self._parse_score(detail_result.detail_score)
        logical_coherence_score = self._parse_score(logical_coherence_result.logical_coherence_score)
        genre_score = self._parse_score(genre_result.genre_score)
        engagement_score = self._parse_score(engagement_result.engagement_score)
        
        # Calculate weighted overall score
        weights = {
            'novelty': 0.18,
            'feasibility': 0.12,
            'structure': 0.08,
            'detail': 0.18,
            'logical_coherence': 0.16,
            'genre': 0.10,
            'engagement': 0.18
        }
        
        overall_score = (
            novelty_score * weights['novelty'] +
            feasibility_score * weights['feasibility'] +
            structure_score * weights['structure'] +
            detail_score * weights['detail'] +
            logical_coherence_score * weights['logical_coherence'] +
            genre_score * weights['genre'] +
            engagement_score * weights['engagement']
        )
        
        # Combine feedback
        combined_feedback = f"""
è¯„ä¼°ç»“æžœè¯¦æƒ…ï¼š

æ–°é¢–æ€§è¯„åˆ†ï¼š{novelty_score}/10
{novelty_result.novelty_feedback}

å¯è¡Œæ€§è¯„åˆ†ï¼š{feasibility_score}/10
{feasibility_result.feasibility_feedback}

ç»“æž„è¯„åˆ†ï¼š{structure_score}/10
{structure_result.structure_feedback}

è¯¦ç»†ç¨‹åº¦è¯„åˆ†ï¼š{detail_score}/10
{detail_result.detail_feedback}

é€»è¾‘è¿žè´¯æ€§è¯„åˆ†ï¼š{logical_coherence_score}/10
{logical_coherence_result.logical_coherence_feedback}

é¢˜æä¸€è‡´æ€§è¯„åˆ†ï¼š{genre_score}/10
{genre_result.genre_feedback}

å¸å¼•åŠ›è¯„åˆ†ï¼š{engagement_score}/10
{engagement_result.engagement_feedback}

æ€»ä½“è¯„åˆ†ï¼š{overall_score:.1f}/10
"""
        
        # Create result
        result = EvaluationResult(
            overall_score=overall_score,
            novelty_score=novelty_score,
            feasibility_score=feasibility_score,
            structure_score=structure_score,
            detail_score=detail_score,
            logical_coherence_score=logical_coherence_score,
            genre_score=genre_score,
            engagement_score=engagement_score,
            feedback=combined_feedback.strip()
        )
        
        # Cache the result
        self.cache[cache_key] = result
        self._save_cache()
        
        return result

    def _parse_score(self, score_str: str) -> float:
        """Parse score string, handling various formats"""
        if isinstance(score_str, (int, float)):
            return float(score_str)
        
        # Try to extract number from string
        import re
        matches = re.findall(r'(\d+(?:\.\d+)?)', str(score_str))
        if matches:
            try:
                score = float(matches[0])
                return min(max(score, 0.0), 10.0)  # Clamp to 0-10 range
            except ValueError:
                pass
        
        print(f"Warning: Could not parse score '{score_str}', defaulting to 5.0")
        return 5.0

# Grouped optimization support
class GroupedEvaluationMetrics:
    """Metrics for grouped optimization of different evaluation aspects"""
    
    def __init__(self, evaluator: StoryIdeaEvaluator):
        self.evaluator = evaluator
        
        # Define evaluation groups
        self.groups = {
            'creativity': ['novelty', 'engagement'],
            'feasibility': ['feasibility', 'structure'], 
            'content_quality': ['detail', 'logical_coherence', 'genre']
        }
        
        # Single group for compatibility with current approach
        self.single_group = {
            'overall': ['novelty', 'feasibility', 'structure', 'detail', 'logical_coherence', 'genre', 'engagement']
        }
    
    def create_group_metric(self, group_name: str, metric_names: List[str]):
        """Create a metric function for a specific group of evaluation criteria"""
        def group_metric(example, prediction, trace=None) -> float:
            """Metric function for a specific group"""
            try:
                # Extract ideas from prediction
                # With the new DSPy approach, prediction should be a list of StoryIdea objects
                if hasattr(prediction, '__iter__') and not isinstance(prediction, str):
                    ideas = list(prediction)
                else:
                    print(f"Warning: Unexpected prediction type: {type(prediction)}")
                    return 0.0
                
                # Create request from example
                request = BrainstormRequest(
                    genre=example.genre,
                    platform=example.platform,
                    requirements_section=getattr(example, 'requirements_section', '')
                )
                
                # Evaluate ideas
                result = self.evaluator.evaluate(ideas, request)
                
                # Calculate group score
                if group_name == 'overall':
                    # Use overall score for single-group optimization (current approach)
                    return result.overall_score / 10.0
                else:
                    # Calculate average of metrics in this group
                    group_scores = []
                    for metric_name in metric_names:
                        score = getattr(result, f'{metric_name}_score', 0)
                        group_scores.append(score)
                    
                    if group_scores:
                        return sum(group_scores) / len(group_scores) / 10.0
                    else:
                        return 0.0
                
            except Exception as e:
                print(f"Group evaluation error ({group_name}): {e}")
                return 0.0
        
        return group_metric
    
    def get_all_group_metrics(self, use_single_group: bool = False) -> Dict[str, callable]:
        """Get all group metrics for optimization"""
        if use_single_group:
            # Return single group (current approach)
            return {
                'overall': self.create_group_metric('overall', self.single_group['overall'])
            }
        else:
            # Return grouped metrics
            metrics = {}
            for group_name, metric_names in self.groups.items():
                metrics[group_name] = self.create_group_metric(group_name, metric_names)
            return metrics

def create_evaluation_metric(evaluator: StoryIdeaEvaluator):
    """Create a metric function for DSPy optimization (backwards compatibility)"""
    grouped_metrics = GroupedEvaluationMetrics(evaluator)
    overall_metrics = grouped_metrics.get_all_group_metrics(use_single_group=True)
    return overall_metrics['overall']

def create_grouped_evaluation_metrics(evaluator: StoryIdeaEvaluator, use_single_group: bool = False):
    """Create grouped evaluation metrics for advanced optimization"""
    grouped_metrics = GroupedEvaluationMetrics(evaluator)
    return grouped_metrics.get_all_group_metrics(use_single_group=use_single_group) 